{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow import constant_initializer\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import shutil\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../code/\")\n",
    "from model import PhasedSNForecastProbabilisticIntervalModel\n",
    "out_steps = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    masked_data = np.ma.masked_where(data < 0, data)\n",
    "    min_val = masked_data.min(axis=1)\n",
    "    max_val = masked_data.max(axis=1)\n",
    "    \n",
    "    for i in range(masked_data.shape[1]):\n",
    "        masked_data.data[:,i,:] = (masked_data.data[:,i,:] - min_val)/(max_val-min_val)\n",
    "    \n",
    "    return_data = masked_data.data\n",
    "    return_data[masked_data.mask] = -1\n",
    "    return return_data, min_val, max_val\n",
    "    \n",
    "def denormalize(data, min_val, max_val):\n",
    "    masked_data = np.ma.masked_where(data < 0, data)\n",
    "    \n",
    "    for i in range(masked_data.shape[1]):\n",
    "        masked_data.data[:,i,:] = (masked_data.data[:,i,:] * (max_val-min_val))  +  min_val\n",
    "    \n",
    "    return_data = masked_data.data\n",
    "    return_data[masked_data.mask] = -1\n",
    "    return return_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"../data/padded_x_train.npy\")\n",
    "len_data = data.shape[1]\n",
    "data, data_min_val, data_max_val = normalize(data)\n",
    "X_train, y_train = data[:,:-out_steps,:],  data[:,-out_steps:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val = np.load(\"../data/padded_x_val.npy\")\n",
    "len_data = data_val.shape[1]\n",
    "data_val, data_val_min_val, data_val_max_val = normalize(data_val)\n",
    "X_val, y_val = data_val[:,:-out_steps,:],  data_val[:,-out_steps:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = X_train\n",
    "outputs = y_train\n",
    "inputs_val = X_val\n",
    "outputs_val = y_val\n",
    "\n",
    "outputs = {}\n",
    "outputs_val = {}\n",
    "\n",
    "outputs[\"prediction\"] = y_train\n",
    "outputs_val[\"prediction\"] = y_val\n",
    "\n",
    "for interval in [\"upper\", \"lower\"]:\n",
    "    outputs[interval] = np.expand_dims(y_train[:,:,1],axis=-1)\n",
    "    outputs_val[interval] = np.expand_dims(y_val[:,:,1],axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveData(tf.keras.callbacks.Callback):\n",
    "    def __init__(self,logdir, keys,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.file_writer = tf.summary.create_file_writer(logdir + \"/metrics\")\n",
    "        self.file_writer.set_as_default()\n",
    "        self.keys = keys\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        for key in self.keys:\n",
    "            tf.summary.scalar(key, data=logs.get(key), step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "#Early stops\n",
    "early_stop = tf.keras.callbacks.EarlyStopping( monitor='val_loss', min_delta=1e-10, patience=10)\n",
    "\n",
    "#Tensorboard\n",
    "logdir = \"../data/training/logs/PI\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(logdir)\n",
    "saver = SaveData(logdir, [\"PICW\"])\n",
    "shutil.rmtree(\"../data/training/logs/PI\",ignore_errors=True)\n",
    "\n",
    "\n",
    "#Checkpoint\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\"../data/training_PI/model_checkpoints/checkpoint\", monitor='val_loss', verbose=0, save_best_only=True)\n",
    "\n",
    "callbacks = [tensorboard,checkpoint, early_stop, saver]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 926us/step - loss: 0.3128\n"
     ]
    }
   ],
   "source": [
    "#Loading and preparing model\n",
    "from model import PhasedSNForecastModel\n",
    "base_model = PhasedSNForecastModel(units=150, out_steps=out_steps,features = 3)\n",
    "base_model.compile(optimizer=\"rmsprop\", loss=\"mse\")\n",
    "_ = base_model.fit(X_train[:2], y_train[:2])\n",
    "\n",
    "\n",
    "base_model.load_weights(\"../data/sn_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PhasedSNForecastProbabilisticIntervalModel(units=300, out_steps=out_steps, model = base_model, dropout=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.rnn.trainable = False\n",
    "model.denses.trainable = False\n",
    "model.cells.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_addons.utils.keras_utils import LossFunctionWrapper\n",
    "from tensorflow_addons.utils.types import TensorLike, FloatTensorLike\n",
    "from typeguard import typechecked\n",
    "\n",
    "@tf.function\n",
    "def custom_pinball_loss(y_true: TensorLike, y_pred: TensorLike, tau: FloatTensorLike = 0.5) -> tf.Tensor:\n",
    "    y_pred = tf.convert_to_tensor(y_pred)\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "    \n",
    "    \n",
    "    # Broadcast the pinball slope along the batch dimension\n",
    "    tau = tf.expand_dims(tf.cast(tau, y_pred.dtype), 0)\n",
    "    one = tf.cast(1, tau.dtype)\n",
    "\n",
    "    pinball = tf.where(y_pred > y_true, tau * (y_pred - y_true), (1-tau) * (y_true-y_pred) )\n",
    "    return tf.reduce_mean(pinball, axis=-1)\n",
    "\n",
    "class CustomPinballLoss(LossFunctionWrapper):\n",
    "    @typechecked\n",
    "    def __init__(\n",
    "        self,\n",
    "        tau: FloatTensorLike = 0.5,\n",
    "        reduction: str = tf.keras.losses.Reduction.AUTO,\n",
    "        name: str = \"custom_pinball_loss\",\n",
    "    ):\n",
    "        super().__init__(custom_pinball_loss, reduction=reduction, name=name, tau=tau)\n",
    "        \n",
    "        \n",
    "@tf.function\n",
    "def inverse_pinball_loss(y_true: TensorLike, y_pred: TensorLike, tau: FloatTensorLike = 0.5) -> tf.Tensor:\n",
    "    y_pred = tf.convert_to_tensor(y_pred)\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "    \n",
    "    \n",
    "    # Broadcast the pinball slope along the batch dimension\n",
    "    tau = tf.expand_dims(tf.cast(tau, y_pred.dtype), 0)\n",
    "    one = tf.cast(1, tau.dtype)\n",
    "\n",
    "    pinball = tf.where(y_pred > y_true, (1-tau) * (y_pred - y_true), tau * (y_true-y_pred) )\n",
    "    return tf.reduce_mean(pinball, axis=-1)   \n",
    "\n",
    "class InversePinballLoss(LossFunctionWrapper):\n",
    "    @typechecked\n",
    "    def __init__(\n",
    "        self,\n",
    "        tau: FloatTensorLike = 0.5,\n",
    "        reduction: str = tf.keras.losses.Reduction.AUTO,\n",
    "        name: str = \"inverse_pinball_loss\",\n",
    "    ):\n",
    "        super().__init__(inverse_pinball_loss, reduction=reduction, name=name, tau=tau)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.30\n",
    "losses = {\n",
    "    \"prediction\": None,\n",
    "    \"lower\": CustomPinballLoss(tau=(alpha/2), reduction=tf.keras.losses.Reduction.NONE),\n",
    "    \"upper\": CustomPinballLoss(tau=1-(alpha/2), reduction=tf.keras.losses.Reduction.NONE)\n",
    "}\n",
    "model.compile(optimizer=\"rmsprop\", loss=losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.5751 - lower_loss: 0.4938 - upper_loss: 0.0813 - PICW: 0.3336WARNING:tensorflow:From /home/camilo/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      " 2/11 [====>.........................] - ETA: 1s - loss: 0.5074 - lower_loss: 0.4325 - upper_loss: 0.0749 - PICW: 0.2640WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1401s vs `on_train_batch_end` time: 0.2217s). Check your callbacks.\n",
      "11/11 [==============================] - 2s 198ms/step - loss: 0.2259 - lower_loss: 0.1721 - upper_loss: 0.0538 - PICW: 11.0128 - val_loss: 0.0872 - val_lower_loss: 0.0481 - val_upper_loss: 0.0391 - val_PICW: 20.5290\n",
      "Epoch 2/1000\n",
      "11/11 [==============================] - 1s 133ms/step - loss: 0.0917 - lower_loss: 0.0490 - upper_loss: 0.0427 - PICW: 20.0562 - val_loss: 0.0807 - val_lower_loss: 0.0430 - val_upper_loss: 0.0377 - val_PICW: 17.2743\n",
      "Epoch 3/1000\n",
      "11/11 [==============================] - 1s 133ms/step - loss: 0.0879 - lower_loss: 0.0461 - upper_loss: 0.0418 - PICW: 18.8289 - val_loss: 0.0799 - val_lower_loss: 0.0424 - val_upper_loss: 0.0375 - val_PICW: 18.5341\n",
      "Epoch 4/1000\n",
      "11/11 [==============================] - 1s 135ms/step - loss: 0.0875 - lower_loss: 0.0449 - upper_loss: 0.0426 - PICW: 18.7940 - val_loss: 0.0774 - val_lower_loss: 0.0394 - val_upper_loss: 0.0380 - val_PICW: 18.8160\n",
      "Epoch 5/1000\n",
      "11/11 [==============================] - 2s 187ms/step - loss: 0.0850 - lower_loss: 0.0440 - upper_loss: 0.0409 - PICW: 18.4166 - val_loss: 0.0764 - val_lower_loss: 0.0391 - val_upper_loss: 0.0373 - val_PICW: 17.2161\n",
      "Epoch 6/1000\n",
      "11/11 [==============================] - 2s 200ms/step - loss: 0.0849 - lower_loss: 0.0442 - upper_loss: 0.0406 - PICW: 18.3104 - val_loss: 0.0773 - val_lower_loss: 0.0400 - val_upper_loss: 0.0372 - val_PICW: 17.5348\n",
      "Epoch 7/1000\n",
      "11/11 [==============================] - 2s 197ms/step - loss: 0.0855 - lower_loss: 0.0444 - upper_loss: 0.0411 - PICW: 18.3527 - val_loss: 0.0759 - val_lower_loss: 0.0382 - val_upper_loss: 0.0378 - val_PICW: 17.2567\n",
      "Epoch 8/1000\n",
      "11/11 [==============================] - 2s 201ms/step - loss: 0.0829 - lower_loss: 0.0420 - upper_loss: 0.0409 - PICW: 18.0020 - val_loss: 0.0781 - val_lower_loss: 0.0389 - val_upper_loss: 0.0392 - val_PICW: 19.9649\n",
      "Epoch 9/1000\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 0.0848 - lower_loss: 0.0443 - upper_loss: 0.0405 - PICW: 18.5674 - val_loss: 0.0769 - val_lower_loss: 0.0384 - val_upper_loss: 0.0385 - val_PICW: 18.4565\n",
      "Epoch 10/1000\n",
      "11/11 [==============================] - 2s 198ms/step - loss: 0.0817 - lower_loss: 0.0420 - upper_loss: 0.0397 - PICW: 17.7569 - val_loss: 0.0784 - val_lower_loss: 0.0399 - val_upper_loss: 0.0386 - val_PICW: 16.6578\n",
      "Epoch 11/1000\n",
      "11/11 [==============================] - 2s 197ms/step - loss: 0.0845 - lower_loss: 0.0426 - upper_loss: 0.0418 - PICW: 18.1829 - val_loss: 0.0776 - val_lower_loss: 0.0385 - val_upper_loss: 0.0392 - val_PICW: 19.1066\n",
      "Epoch 12/1000\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 0.0826 - lower_loss: 0.0424 - upper_loss: 0.0401 - PICW: 18.0982 - val_loss: 0.0768 - val_lower_loss: 0.0391 - val_upper_loss: 0.0377 - val_PICW: 17.3215\n",
      "Epoch 13/1000\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 0.0819 - lower_loss: 0.0420 - upper_loss: 0.0400 - PICW: 17.9700 - val_loss: 0.0761 - val_lower_loss: 0.0380 - val_upper_loss: 0.0381 - val_PICW: 16.3567\n",
      "Epoch 14/1000\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 0.0826 - lower_loss: 0.0419 - upper_loss: 0.0407 - PICW: 18.0070 - val_loss: 0.0760 - val_lower_loss: 0.0376 - val_upper_loss: 0.0383 - val_PICW: 14.4261\n",
      "Epoch 15/1000\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 0.0810 - lower_loss: 0.0411 - upper_loss: 0.0398 - PICW: 17.4283 - val_loss: 0.0760 - val_lower_loss: 0.0376 - val_upper_loss: 0.0383 - val_PICW: 16.6945\n",
      "Epoch 16/1000\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 0.0817 - lower_loss: 0.0407 - upper_loss: 0.0410 - PICW: 17.9404 - val_loss: 0.0756 - val_lower_loss: 0.0374 - val_upper_loss: 0.0382 - val_PICW: 15.1939\n",
      "Epoch 17/1000\n",
      "11/11 [==============================] - 2s 199ms/step - loss: 0.0811 - lower_loss: 0.0408 - upper_loss: 0.0403 - PICW: 17.6936 - val_loss: 0.0772 - val_lower_loss: 0.0386 - val_upper_loss: 0.0386 - val_PICW: 16.3783\n",
      "Epoch 18/1000\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 0.0821 - lower_loss: 0.0415 - upper_loss: 0.0406 - PICW: 17.9002 - val_loss: 0.0761 - val_lower_loss: 0.0378 - val_upper_loss: 0.0383 - val_PICW: 16.2544\n",
      "Epoch 19/1000\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 0.0797 - lower_loss: 0.0397 - upper_loss: 0.0400 - PICW: 17.5082 - val_loss: 0.0756 - val_lower_loss: 0.0370 - val_upper_loss: 0.0386 - val_PICW: 15.9573\n",
      "Epoch 20/1000\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 0.0810 - lower_loss: 0.0411 - upper_loss: 0.0399 - PICW: 17.9550 - val_loss: 0.0761 - val_lower_loss: 0.0372 - val_upper_loss: 0.0388 - val_PICW: 14.9160\n",
      "Epoch 21/1000\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 0.0801 - lower_loss: 0.0398 - upper_loss: 0.0403 - PICW: 17.5926 - val_loss: 0.0753 - val_lower_loss: 0.0372 - val_upper_loss: 0.0381 - val_PICW: 15.6571\n",
      "Epoch 22/1000\n",
      "11/11 [==============================] - 2s 197ms/step - loss: 0.0815 - lower_loss: 0.0411 - upper_loss: 0.0404 - PICW: 17.9115 - val_loss: 0.0750 - val_lower_loss: 0.0369 - val_upper_loss: 0.0380 - val_PICW: 15.0137\n",
      "Epoch 23/1000\n",
      "11/11 [==============================] - 2s 200ms/step - loss: 0.0792 - lower_loss: 0.0391 - upper_loss: 0.0401 - PICW: 17.2534 - val_loss: 0.0763 - val_lower_loss: 0.0371 - val_upper_loss: 0.0392 - val_PICW: 14.4149\n",
      "Epoch 24/1000\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 0.0811 - lower_loss: 0.0406 - upper_loss: 0.0405 - PICW: 17.8002 - val_loss: 0.0750 - val_lower_loss: 0.0367 - val_upper_loss: 0.0383 - val_PICW: 15.0851\n",
      "Epoch 25/1000\n",
      "11/11 [==============================] - 2s 195ms/step - loss: 0.0789 - lower_loss: 0.0396 - upper_loss: 0.0393 - PICW: 17.3686 - val_loss: 0.0754 - val_lower_loss: 0.0373 - val_upper_loss: 0.0381 - val_PICW: 16.7897\n",
      "Epoch 26/1000\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 0.0801 - lower_loss: 0.0398 - upper_loss: 0.0404 - PICW: 17.3732 - val_loss: 0.0782 - val_lower_loss: 0.0399 - val_upper_loss: 0.0383 - val_PICW: 18.8705\n",
      "Epoch 27/1000\n",
      "11/11 [==============================] - 2s 199ms/step - loss: 0.0781 - lower_loss: 0.0387 - upper_loss: 0.0394 - PICW: 17.4758 - val_loss: 0.0741 - val_lower_loss: 0.0360 - val_upper_loss: 0.0381 - val_PICW: 15.6048\n",
      "Epoch 28/1000\n",
      "11/11 [==============================] - 2s 195ms/step - loss: 0.0798 - lower_loss: 0.0397 - upper_loss: 0.0401 - PICW: 17.7457 - val_loss: 0.0751 - val_lower_loss: 0.0359 - val_upper_loss: 0.0391 - val_PICW: 14.6871\n",
      "Epoch 29/1000\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 0.0783 - lower_loss: 0.0385 - upper_loss: 0.0398 - PICW: 17.2248 - val_loss: 0.0746 - val_lower_loss: 0.0362 - val_upper_loss: 0.0384 - val_PICW: 17.1597\n",
      "Epoch 30/1000\n",
      "11/11 [==============================] - 2s 195ms/step - loss: 0.0773 - lower_loss: 0.0384 - upper_loss: 0.0389 - PICW: 17.1827 - val_loss: 0.0746 - val_lower_loss: 0.0366 - val_upper_loss: 0.0380 - val_PICW: 16.6652\n",
      "Epoch 31/1000\n",
      "11/11 [==============================] - 2s 197ms/step - loss: 0.0786 - lower_loss: 0.0386 - upper_loss: 0.0400 - PICW: 17.4383 - val_loss: 0.0741 - val_lower_loss: 0.0355 - val_upper_loss: 0.0386 - val_PICW: 14.3940\n",
      "Epoch 32/1000\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 0.0774 - lower_loss: 0.0384 - upper_loss: 0.0390 - PICW: 17.2386 - val_loss: 0.0739 - val_lower_loss: 0.0352 - val_upper_loss: 0.0387 - val_PICW: 13.6010\n",
      "Epoch 33/1000\n",
      "11/11 [==============================] - 2s 195ms/step - loss: 0.0783 - lower_loss: 0.0386 - upper_loss: 0.0397 - PICW: 17.3342 - val_loss: 0.0740 - val_lower_loss: 0.0361 - val_upper_loss: 0.0380 - val_PICW: 16.4330\n",
      "Epoch 34/1000\n",
      "11/11 [==============================] - 2s 199ms/step - loss: 0.0770 - lower_loss: 0.0382 - upper_loss: 0.0389 - PICW: 17.2091 - val_loss: 0.0726 - val_lower_loss: 0.0347 - val_upper_loss: 0.0378 - val_PICW: 14.5263\n",
      "Epoch 35/1000\n",
      "11/11 [==============================] - 2s 199ms/step - loss: 0.0770 - lower_loss: 0.0382 - upper_loss: 0.0388 - PICW: 17.1837 - val_loss: 0.0720 - val_lower_loss: 0.0345 - val_upper_loss: 0.0375 - val_PICW: 15.3551\n",
      "Epoch 36/1000\n",
      "11/11 [==============================] - 2s 198ms/step - loss: 0.0765 - lower_loss: 0.0377 - upper_loss: 0.0388 - PICW: 16.9821 - val_loss: 0.0731 - val_lower_loss: 0.0348 - val_upper_loss: 0.0382 - val_PICW: 14.4467\n",
      "Epoch 37/1000\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 0.0755 - lower_loss: 0.0367 - upper_loss: 0.0388 - PICW: 16.8116 - val_loss: 0.0728 - val_lower_loss: 0.0357 - val_upper_loss: 0.0372 - val_PICW: 17.4348\n",
      "Epoch 38/1000\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 0.0766 - lower_loss: 0.0377 - upper_loss: 0.0389 - PICW: 17.1330 - val_loss: 0.0724 - val_lower_loss: 0.0351 - val_upper_loss: 0.0373 - val_PICW: 15.4565\n",
      "Epoch 39/1000\n",
      "11/11 [==============================] - 2s 197ms/step - loss: 0.0762 - lower_loss: 0.0377 - upper_loss: 0.0385 - PICW: 17.1904 - val_loss: 0.0712 - val_lower_loss: 0.0341 - val_upper_loss: 0.0370 - val_PICW: 15.3808\n",
      "Epoch 40/1000\n",
      "11/11 [==============================] - 2s 195ms/step - loss: 0.0749 - lower_loss: 0.0370 - upper_loss: 0.0380 - PICW: 16.7765 - val_loss: 0.0720 - val_lower_loss: 0.0344 - val_upper_loss: 0.0376 - val_PICW: 15.4286\n",
      "Epoch 41/1000\n",
      "11/11 [==============================] - 2s 197ms/step - loss: 0.0760 - lower_loss: 0.0374 - upper_loss: 0.0386 - PICW: 17.4135 - val_loss: 0.0718 - val_lower_loss: 0.0339 - val_upper_loss: 0.0379 - val_PICW: 13.4233\n",
      "Epoch 42/1000\n",
      "11/11 [==============================] - 2s 195ms/step - loss: 0.0745 - lower_loss: 0.0364 - upper_loss: 0.0380 - PICW: 16.6511 - val_loss: 0.0725 - val_lower_loss: 0.0351 - val_upper_loss: 0.0374 - val_PICW: 16.0170\n",
      "Epoch 43/1000\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 0.0748 - lower_loss: 0.0366 - upper_loss: 0.0382 - PICW: 16.9797 - val_loss: 0.0710 - val_lower_loss: 0.0342 - val_upper_loss: 0.0369 - val_PICW: 15.2330\n",
      "Epoch 44/1000\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 0.0755 - lower_loss: 0.0375 - upper_loss: 0.0380 - PICW: 16.8330 - val_loss: 0.0722 - val_lower_loss: 0.0344 - val_upper_loss: 0.0377 - val_PICW: 14.7927\n",
      "Epoch 45/1000\n",
      "11/11 [==============================] - 2s 198ms/step - loss: 0.0752 - lower_loss: 0.0366 - upper_loss: 0.0387 - PICW: 16.7594 - val_loss: 0.0709 - val_lower_loss: 0.0342 - val_upper_loss: 0.0366 - val_PICW: 16.0195\n",
      "Epoch 46/1000\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 0.0741 - lower_loss: 0.0359 - upper_loss: 0.0381 - PICW: 16.7025 - val_loss: 0.0714 - val_lower_loss: 0.0345 - val_upper_loss: 0.0369 - val_PICW: 16.2753\n",
      "Epoch 47/1000\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 0.0734 - lower_loss: 0.0358 - upper_loss: 0.0376 - PICW: 16.6051 - val_loss: 0.0712 - val_lower_loss: 0.0339 - val_upper_loss: 0.0373 - val_PICW: 15.3729\n",
      "Epoch 48/1000\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 0.0745 - lower_loss: 0.0364 - upper_loss: 0.0382 - PICW: 16.8648 - val_loss: 0.0709 - val_lower_loss: 0.0339 - val_upper_loss: 0.0370 - val_PICW: 15.2847\n",
      "Epoch 49/1000\n",
      "11/11 [==============================] - 2s 201ms/step - loss: 0.0749 - lower_loss: 0.0368 - upper_loss: 0.0381 - PICW: 16.9949 - val_loss: 0.0708 - val_lower_loss: 0.0336 - val_upper_loss: 0.0373 - val_PICW: 14.7733\n",
      "Epoch 50/1000\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 0.0752 - lower_loss: 0.0364 - upper_loss: 0.0388 - PICW: 17.0856 - val_loss: 0.0707 - val_lower_loss: 0.0336 - val_upper_loss: 0.0371 - val_PICW: 14.4484\n",
      "Epoch 51/1000\n",
      "11/11 [==============================] - 2s 199ms/step - loss: 0.0735 - lower_loss: 0.0358 - upper_loss: 0.0377 - PICW: 16.4570 - val_loss: 0.0709 - val_lower_loss: 0.0332 - val_upper_loss: 0.0377 - val_PICW: 13.9466\n",
      "Epoch 52/1000\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 0.0741 - lower_loss: 0.0360 - upper_loss: 0.0382 - PICW: 16.6157 - val_loss: 0.0710 - val_lower_loss: 0.0332 - val_upper_loss: 0.0378 - val_PICW: 13.3550\n",
      "Epoch 53/1000\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 0.0741 - lower_loss: 0.0362 - upper_loss: 0.0379 - PICW: 16.5682 - val_loss: 0.0699 - val_lower_loss: 0.0331 - val_upper_loss: 0.0368 - val_PICW: 14.3298\n",
      "Epoch 54/1000\n",
      "11/11 [==============================] - 2s 195ms/step - loss: 0.0739 - lower_loss: 0.0356 - upper_loss: 0.0382 - PICW: 16.5526 - val_loss: 0.0704 - val_lower_loss: 0.0333 - val_upper_loss: 0.0371 - val_PICW: 14.6237\n",
      "Epoch 55/1000\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 0.0742 - lower_loss: 0.0363 - upper_loss: 0.0379 - PICW: 16.6505 - val_loss: 0.0703 - val_lower_loss: 0.0331 - val_upper_loss: 0.0372 - val_PICW: 16.3998\n",
      "Epoch 56/1000\n",
      "11/11 [==============================] - 2s 193ms/step - loss: 0.0739 - lower_loss: 0.0360 - upper_loss: 0.0380 - PICW: 16.6199 - val_loss: 0.0709 - val_lower_loss: 0.0339 - val_upper_loss: 0.0370 - val_PICW: 14.7504\n",
      "Epoch 57/1000\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 0.0735 - lower_loss: 0.0357 - upper_loss: 0.0377 - PICW: 16.6076 - val_loss: 0.0698 - val_lower_loss: 0.0331 - val_upper_loss: 0.0368 - val_PICW: 15.2930\n",
      "Epoch 58/1000\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 0.0738 - lower_loss: 0.0358 - upper_loss: 0.0380 - PICW: 16.4611 - val_loss: 0.0703 - val_lower_loss: 0.0335 - val_upper_loss: 0.0368 - val_PICW: 15.6944\n",
      "Epoch 59/1000\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 0.0735 - lower_loss: 0.0354 - upper_loss: 0.0381 - PICW: 16.7240 - val_loss: 0.0699 - val_lower_loss: 0.0332 - val_upper_loss: 0.0366 - val_PICW: 15.5361\n",
      "Epoch 60/1000\n",
      "11/11 [==============================] - 2s 206ms/step - loss: 0.0734 - lower_loss: 0.0356 - upper_loss: 0.0378 - PICW: 16.5230 - val_loss: 0.0699 - val_lower_loss: 0.0330 - val_upper_loss: 0.0369 - val_PICW: 15.2589\n",
      "Epoch 61/1000\n",
      "11/11 [==============================] - 2s 213ms/step - loss: 0.0730 - lower_loss: 0.0352 - upper_loss: 0.0378 - PICW: 16.6507 - val_loss: 0.0705 - val_lower_loss: 0.0337 - val_upper_loss: 0.0368 - val_PICW: 13.9924\n",
      "Epoch 62/1000\n",
      "11/11 [==============================] - 2s 195ms/step - loss: 0.0736 - lower_loss: 0.0358 - upper_loss: 0.0378 - PICW: 16.4261 - val_loss: 0.0702 - val_lower_loss: 0.0336 - val_upper_loss: 0.0366 - val_PICW: 15.2891\n",
      "Epoch 63/1000\n",
      "11/11 [==============================] - 2s 199ms/step - loss: 0.0730 - lower_loss: 0.0351 - upper_loss: 0.0379 - PICW: 16.5936 - val_loss: 0.0704 - val_lower_loss: 0.0333 - val_upper_loss: 0.0371 - val_PICW: 14.2893\n",
      "Epoch 64/1000\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 0.0738 - lower_loss: 0.0359 - upper_loss: 0.0379 - PICW: 16.7504 - val_loss: 0.0703 - val_lower_loss: 0.0331 - val_upper_loss: 0.0372 - val_PICW: 14.7903\n",
      "Epoch 65/1000\n",
      "11/11 [==============================] - 2s 197ms/step - loss: 0.0740 - lower_loss: 0.0357 - upper_loss: 0.0383 - PICW: 16.5294 - val_loss: 0.0702 - val_lower_loss: 0.0332 - val_upper_loss: 0.0370 - val_PICW: 14.6142\n",
      "Epoch 66/1000\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 0.0738 - lower_loss: 0.0356 - upper_loss: 0.0382 - PICW: 16.5137 - val_loss: 0.0710 - val_lower_loss: 0.0341 - val_upper_loss: 0.0370 - val_PICW: 16.1103\n",
      "Epoch 67/1000\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 0.0729 - lower_loss: 0.0350 - upper_loss: 0.0378 - PICW: 16.4433 - val_loss: 0.0708 - val_lower_loss: 0.0336 - val_upper_loss: 0.0373 - val_PICW: 15.6091\n"
     ]
    }
   ],
   "source": [
    "MAX_EPOCHS=1000\n",
    "history = model.fit(inputs,outputs,\n",
    "                    batch_size=300, \n",
    "                    epochs=MAX_EPOCHS, \n",
    "                    validation_data=(inputs_val,outputs_val), \n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "json.dump(history_dict, open(\"../data/training_PI/history_model.json\", 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"../data/sn_model_PI.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = np.load(\"../data/padded_x_val.npy\")[:,:,:]\n",
    "data_test, data_test_min_val, data_test_max_val = normalize(data_test)\n",
    "X_test, y_test = data_test[:,:-out_steps,:], data_test[:,-out_steps:, :]\n",
    "\n",
    "#Doing inference on Train data\n",
    "y_hat_train = model.predict(X_train)\n",
    "#Denormalizing train\n",
    "dX_train = denormalize(X_train, data_min_val,data_max_val)\n",
    "dy_hat_train = {}\n",
    "dy_hat_train[\"prediction\"] = denormalize(y_hat_train[\"prediction\"], data_min_val,data_max_val)\n",
    "for key in [\"upper\", \"lower\"]:\n",
    "    dy_hat_train[key] = denormalize(y_hat_train[key], data_min_val[:,1][:,np.newaxis],data_max_val[:,1][:,np.newaxis])\n",
    "dy_train = denormalize(y_train, data_min_val,data_max_val)\n",
    "\n",
    "# Doing inference on Test data\n",
    "y_hat = model.predict(X_test)\n",
    "# Denormalizing results\n",
    "dX_test = denormalize(X_test, data_test_min_val,data_test_max_val)\n",
    "dy_hat = {}\n",
    "dy_hat[\"prediction\"] = denormalize(y_hat[\"prediction\"],data_test_min_val,data_test_max_val) \n",
    "for key in [\"upper\", \"lower\"]:\n",
    "    dy_hat[key] = denormalize(y_hat[key],data_test_min_val[:,1][:,np.newaxis],data_test_max_val[:,1][:,np.newaxis])\n",
    "dy_test = denormalize(y_test,data_test_min_val,data_test_max_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e6b172b503d40429e9a8a3162e03024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=397, description='sample', max=795), Output()), _dom_classes=('widget-inâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(sample)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plot_data(x, y_real, y_hat, sample=0):\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.gca().invert_yaxis()\n",
    "    x_masked = np.ma.masked_where(x < 0, x)\n",
    "    plt.scatter(x_masked[sample,:,0], x_masked[sample,:,1], label=\"History\")\n",
    "    plt.scatter(y_real[sample,:,0], y_real[sample,:,1], label=\"Real\")\n",
    "    plt.scatter(y_hat[\"prediction\"][sample,:,0], y_hat[\"prediction\"][sample,:,1], label=\"Prediction\")\n",
    "    plt.fill_between(y_hat[\"prediction\"][sample,:,0], y_hat[\"lower\"][sample,:,0], y_hat[\"upper\"][sample,:,0], alpha=0.2)\n",
    "    plt.xlabel(\"Time $mjd-\\min(mjd)$\")\n",
    "    plt.ylabel(\"Mag\")\n",
    "    \n",
    "    \n",
    "\n",
    "f = lambda sample: plot_data(dX_test, dy_test, dy_hat,sample=sample)\n",
    "interact(f, sample=(0,len(dX_test)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (796 of 796) |######################| Elapsed Time: 0:02:17 ETA:  00:00:00"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import progressbar\n",
    "bar = progressbar.ProgressBar(max_value=len(X_test))\n",
    "os.makedirs(\"../data/plots_test_PI/\",exist_ok=True)\n",
    "\n",
    "x = dX_test\n",
    "y_real = dy_test\n",
    "y_hat = dy_hat\n",
    "bar.start()\n",
    "for sample in range(len(dX_test)):\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.gca().invert_yaxis()\n",
    "    x_masked = np.ma.masked_where(x < 0, x)\n",
    "    plt.scatter(x_masked[sample,:,0], x_masked[sample,:,1], label=\"History\")\n",
    "    plt.scatter(y_real[sample,:,0], y_real[sample,:,1], label=\"Real\")\n",
    "    plt.scatter(y_hat[\"prediction\"][sample,:,0], y_hat[\"prediction\"][sample,:,1], label=\"Prediction\")\n",
    "    plt.fill_between(y_hat[\"prediction\"][sample,:,0], y_hat[\"lower\"][sample,:,0], y_hat[\"upper\"][sample,:,0], alpha=0.2)\n",
    "    plt.xlabel(\"Time $mjd-\\min(mjd)$\")\n",
    "    plt.ylabel(\"Mag\")\n",
    "    plt.savefig(f\"../data/plots_test_PI/{str(sample).rjust(5,'0')}\")\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "    bar.update(sample+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
