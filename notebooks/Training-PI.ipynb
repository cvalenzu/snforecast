{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow import constant_initializer\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import shutil\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../code/\")\n",
    "from model import PhasedSNForecastProbabilisticIntervalModel\n",
    "out_steps = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    masked_data = np.ma.masked_where(data < 0, data)\n",
    "    min_val = masked_data.min(axis=1)\n",
    "    max_val = masked_data.max(axis=1)\n",
    "    \n",
    "    for i in range(masked_data.shape[1]):\n",
    "        masked_data.data[:,i,:] = (masked_data.data[:,i,:] - min_val)/(max_val-min_val)\n",
    "    \n",
    "    return_data = masked_data.data\n",
    "    return_data[masked_data.mask] = -1\n",
    "    return return_data, min_val, max_val\n",
    "    \n",
    "def denormalize(data, min_val, max_val):\n",
    "    masked_data = np.ma.masked_where(data < 0, data)\n",
    "    \n",
    "    for i in range(masked_data.shape[1]):\n",
    "        masked_data.data[:,i,:] = (masked_data.data[:,i,:] * (max_val-min_val))  +  min_val\n",
    "    \n",
    "    return_data = masked_data.data\n",
    "    return_data[masked_data.mask] = -1\n",
    "    return return_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"../data/padded_x_train.npy\")\n",
    "len_data = data.shape[1]\n",
    "data, data_min_val, data_max_val = normalize(data)\n",
    "X_train, y_train = data[:,:-out_steps,:],  data[:,-out_steps:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val = np.load(\"../data/padded_x_val.npy\")\n",
    "len_data = data_val.shape[1]\n",
    "data_val, data_val_min_val, data_val_max_val = normalize(data_val)\n",
    "X_val, y_val = data_val[:,:-out_steps,:],  data_val[:,-out_steps:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = X_train\n",
    "outputs = y_train\n",
    "inputs_val = X_val\n",
    "outputs_val = y_val\n",
    "\n",
    "outputs = {}\n",
    "outputs_val = {}\n",
    "\n",
    "outputs[\"prediction\"] = y_train\n",
    "outputs_val[\"prediction\"] = y_val\n",
    "\n",
    "for interval in [\"upper\", \"lower\"]:\n",
    "    outputs[interval] = np.expand_dims(y_train[:,:,1],axis=-1)\n",
    "    outputs_val[interval] = np.expand_dims(y_val[:,:,1],axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveData(tf.keras.callbacks.Callback):\n",
    "    def __init__(self,logdir, keys,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.file_writer = tf.summary.create_file_writer(logdir + \"/metrics\")\n",
    "        self.file_writer.set_as_default()\n",
    "        self.keys = keys\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        for key in self.keys:\n",
    "            tf.summary.scalar(key, data=logs.get(key), step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "#Early stops\n",
    "early_stop = tf.keras.callbacks.EarlyStopping( monitor='val_loss', min_delta=1e-10, patience=10)\n",
    "\n",
    "#Tensorboard\n",
    "logdir = \"../data/training_PI/logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(logdir)\n",
    "saver = SaveData(logdir, [\"PICW\"])\n",
    "shutil.rmtree(\"../data/training_PI/logs\",ignore_errors=True)\n",
    "\n",
    "\n",
    "#Checkpoint\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\"../data/training_PI/model_checkpoints/checkpoint\", monitor='val_loss', verbose=0, save_best_only=True)\n",
    "\n",
    "callbacks = [tensorboard,checkpoint, early_stop, saver]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1712\n"
     ]
    }
   ],
   "source": [
    "#Loading and preparing model\n",
    "from model import PhasedSNForecastModel\n",
    "base_model = PhasedSNForecastModel(units=150, out_steps=out_steps,features = 3)\n",
    "base_model.compile(optimizer=\"rmsprop\", loss=\"mse\")\n",
    "_ = base_model.fit(X_train[:2], y_train[:2])\n",
    "\n",
    "\n",
    "base_model.load_weights(\"../data/sn_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PhasedSNForecastProbabilisticIntervalModel(units=300, out_steps=out_steps, model = base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.rnn.trainable = False\n",
    "model.denses.trainable = False\n",
    "model.cells.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_addons.utils.keras_utils import LossFunctionWrapper\n",
    "from tensorflow_addons.utils.types import TensorLike, FloatTensorLike\n",
    "from typeguard import typechecked\n",
    "\n",
    "@tf.function\n",
    "def custom_pinball_loss(y_true: TensorLike, y_pred: TensorLike, tau: FloatTensorLike = 0.5) -> tf.Tensor:\n",
    "    y_pred = tf.convert_to_tensor(y_pred)\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "    \n",
    "    \n",
    "    # Broadcast the pinball slope along the batch dimension\n",
    "    tau = tf.expand_dims(tf.cast(tau, y_pred.dtype), 0)\n",
    "    one = tf.cast(1, tau.dtype)\n",
    "\n",
    "    pinball = tf.where(y_pred > y_true, tau * (y_pred - y_true), (1-tau) * (y_true-y_pred) )\n",
    "    return tf.reduce_mean(pinball, axis=-1)\n",
    "\n",
    "class CustomPinballLoss(LossFunctionWrapper):\n",
    "    @typechecked\n",
    "    def __init__(\n",
    "        self,\n",
    "        tau: FloatTensorLike = 0.5,\n",
    "        reduction: str = tf.keras.losses.Reduction.AUTO,\n",
    "        name: str = \"custom_pinball_loss\",\n",
    "    ):\n",
    "        super().__init__(custom_pinball_loss, reduction=reduction, name=name, tau=tau)\n",
    "        \n",
    "        \n",
    "@tf.function\n",
    "def inverse_pinball_loss(y_true: TensorLike, y_pred: TensorLike, tau: FloatTensorLike = 0.5) -> tf.Tensor:\n",
    "    y_pred = tf.convert_to_tensor(y_pred)\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "    \n",
    "    \n",
    "    # Broadcast the pinball slope along the batch dimension\n",
    "    tau = tf.expand_dims(tf.cast(tau, y_pred.dtype), 0)\n",
    "    one = tf.cast(1, tau.dtype)\n",
    "\n",
    "    pinball = tf.where(y_pred > y_true, (1-tau) * (y_pred - y_true), tau * (y_true-y_pred) )\n",
    "    return tf.reduce_mean(pinball, axis=-1)   \n",
    "\n",
    "class InversePinballLoss(LossFunctionWrapper):\n",
    "    @typechecked\n",
    "    def __init__(\n",
    "        self,\n",
    "        tau: FloatTensorLike = 0.5,\n",
    "        reduction: str = tf.keras.losses.Reduction.AUTO,\n",
    "        name: str = \"inverse_pinball_loss\",\n",
    "    ):\n",
    "        super().__init__(inverse_pinball_loss, reduction=reduction, name=name, tau=tau)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.30\n",
    "losses = {\n",
    "    \"prediction\": None,\n",
    "    \"lower\": CustomPinballLoss(tau=(alpha/2), reduction=tf.keras.losses.Reduction.NONE),\n",
    "    \"upper\": CustomPinballLoss(tau=1-(alpha/2), reduction=tf.keras.losses.Reduction.NONE)\n",
    "}\n",
    "model.compile(optimizer=\"rmsprop\", loss=losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 0.4449 - lower_loss: 0.3747 - upper_loss: 0.0702 - PICW: 2.4485WARNING:tensorflow:From /home/camilo/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      " 2/11 [====>.........................] - ETA: 2s - loss: 0.3915 - lower_loss: 0.3233 - upper_loss: 0.0682 - PICW: 3.2305WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2107s vs `on_train_batch_end` time: 0.3765s). Check your callbacks.\n",
      "11/11 [==============================] - 4s 369ms/step - loss: 0.2316 - lower_loss: 0.1699 - upper_loss: 0.0617 - PICW: 18.3088 - val_loss: 0.1163 - val_lower_loss: 0.0637 - val_upper_loss: 0.0526 - val_PICW: 29.6351\n",
      "Epoch 2/1000\n",
      "11/11 [==============================] - 2s 216ms/step - loss: 0.1482 - lower_loss: 0.0949 - upper_loss: 0.0533 - PICW: 29.3474 - val_loss: 0.1091 - val_lower_loss: 0.0606 - val_upper_loss: 0.0485 - val_PICW: 29.0054\n",
      "Epoch 3/1000\n",
      "11/11 [==============================] - 3s 235ms/step - loss: 0.1388 - lower_loss: 0.0870 - upper_loss: 0.0519 - PICW: 27.9074 - val_loss: 0.1055 - val_lower_loss: 0.0575 - val_upper_loss: 0.0480 - val_PICW: 27.9536\n",
      "Epoch 4/1000\n",
      "11/11 [==============================] - 4s 327ms/step - loss: 0.1323 - lower_loss: 0.0812 - upper_loss: 0.0511 - PICW: 26.7386 - val_loss: 0.1036 - val_lower_loss: 0.0557 - val_upper_loss: 0.0479 - val_PICW: 26.0130\n",
      "Epoch 5/1000\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 0.1271 - lower_loss: 0.0766 - upper_loss: 0.0505 - PICW: 26.1577 - val_loss: 0.1020 - val_lower_loss: 0.0543 - val_upper_loss: 0.0477 - val_PICW: 25.9614\n",
      "Epoch 6/1000\n",
      "11/11 [==============================] - 2s 208ms/step - loss: 0.1224 - lower_loss: 0.0732 - upper_loss: 0.0492 - PICW: 25.4052 - val_loss: 0.1039 - val_lower_loss: 0.0557 - val_upper_loss: 0.0482 - val_PICW: 25.7338\n",
      "Epoch 7/1000\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 0.1224 - lower_loss: 0.0724 - upper_loss: 0.0500 - PICW: 25.5896 - val_loss: 0.1015 - val_lower_loss: 0.0544 - val_upper_loss: 0.0471 - val_PICW: 26.8584\n",
      "Epoch 8/1000\n",
      "11/11 [==============================] - 3s 230ms/step - loss: 0.1186 - lower_loss: 0.0695 - upper_loss: 0.0491 - PICW: 24.9412 - val_loss: 0.1006 - val_lower_loss: 0.0527 - val_upper_loss: 0.0479 - val_PICW: 23.9062\n",
      "Epoch 9/1000\n",
      "11/11 [==============================] - 2s 224ms/step - loss: 0.1175 - lower_loss: 0.0686 - upper_loss: 0.0489 - PICW: 24.8444 - val_loss: 0.0989 - val_lower_loss: 0.0518 - val_upper_loss: 0.0472 - val_PICW: 24.4093\n",
      "Epoch 10/1000\n",
      "11/11 [==============================] - 2s 226ms/step - loss: 0.1163 - lower_loss: 0.0676 - upper_loss: 0.0487 - PICW: 24.5456 - val_loss: 0.0990 - val_lower_loss: 0.0520 - val_upper_loss: 0.0471 - val_PICW: 24.4111\n",
      "Epoch 11/1000\n",
      "11/11 [==============================] - 2s 224ms/step - loss: 0.1145 - lower_loss: 0.0666 - upper_loss: 0.0479 - PICW: 24.3240 - val_loss: 0.0984 - val_lower_loss: 0.0515 - val_upper_loss: 0.0469 - val_PICW: 23.8149\n",
      "Epoch 12/1000\n",
      "11/11 [==============================] - 3s 264ms/step - loss: 0.1150 - lower_loss: 0.0662 - upper_loss: 0.0487 - PICW: 24.4377 - val_loss: 0.0981 - val_lower_loss: 0.0514 - val_upper_loss: 0.0468 - val_PICW: 24.0608\n",
      "Epoch 13/1000\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 0.1128 - lower_loss: 0.0650 - upper_loss: 0.0479 - PICW: 23.8875 - val_loss: 0.1006 - val_lower_loss: 0.0528 - val_upper_loss: 0.0478 - val_PICW: 24.4996\n",
      "Epoch 14/1000\n",
      "11/11 [==============================] - 3s 269ms/step - loss: 0.1136 - lower_loss: 0.0654 - upper_loss: 0.0482 - PICW: 24.1757 - val_loss: 0.0985 - val_lower_loss: 0.0516 - val_upper_loss: 0.0469 - val_PICW: 23.9363\n",
      "Epoch 15/1000\n",
      "11/11 [==============================] - 4s 343ms/step - loss: 0.1116 - lower_loss: 0.0638 - upper_loss: 0.0478 - PICW: 23.7911 - val_loss: 0.1019 - val_lower_loss: 0.0534 - val_upper_loss: 0.0485 - val_PICW: 24.6564\n",
      "Epoch 16/1000\n",
      "11/11 [==============================] - 4s 345ms/step - loss: 0.1109 - lower_loss: 0.0630 - upper_loss: 0.0479 - PICW: 23.8978 - val_loss: 0.1025 - val_lower_loss: 0.0540 - val_upper_loss: 0.0485 - val_PICW: 25.2374\n",
      "Epoch 17/1000\n",
      "11/11 [==============================] - 3s 269ms/step - loss: 0.1104 - lower_loss: 0.0630 - upper_loss: 0.0473 - PICW: 23.5399 - val_loss: 0.0980 - val_lower_loss: 0.0511 - val_upper_loss: 0.0470 - val_PICW: 23.4973\n",
      "Epoch 18/1000\n",
      "11/11 [==============================] - 3s 298ms/step - loss: 0.1104 - lower_loss: 0.0631 - upper_loss: 0.0473 - PICW: 23.7531 - val_loss: 0.0975 - val_lower_loss: 0.0510 - val_upper_loss: 0.0465 - val_PICW: 23.2174\n",
      "Epoch 19/1000\n",
      "11/11 [==============================] - 3s 233ms/step - loss: 0.1083 - lower_loss: 0.0616 - upper_loss: 0.0467 - PICW: 23.1986 - val_loss: 0.0972 - val_lower_loss: 0.0508 - val_upper_loss: 0.0465 - val_PICW: 22.7621\n",
      "Epoch 20/1000\n",
      "11/11 [==============================] - 3s 233ms/step - loss: 0.1087 - lower_loss: 0.0614 - upper_loss: 0.0473 - PICW: 23.5301 - val_loss: 0.0982 - val_lower_loss: 0.0509 - val_upper_loss: 0.0472 - val_PICW: 21.9322\n",
      "Epoch 21/1000\n",
      "11/11 [==============================] - 3s 235ms/step - loss: 0.1087 - lower_loss: 0.0617 - upper_loss: 0.0470 - PICW: 23.3300 - val_loss: 0.0977 - val_lower_loss: 0.0510 - val_upper_loss: 0.0467 - val_PICW: 23.2587\n",
      "Epoch 22/1000\n",
      "11/11 [==============================] - 3s 249ms/step - loss: 0.1081 - lower_loss: 0.0608 - upper_loss: 0.0473 - PICW: 23.3987 - val_loss: 0.1000 - val_lower_loss: 0.0527 - val_upper_loss: 0.0473 - val_PICW: 24.4276\n",
      "Epoch 23/1000\n",
      "11/11 [==============================] - 3s 316ms/step - loss: 0.1079 - lower_loss: 0.0610 - upper_loss: 0.0469 - PICW: 23.2000 - val_loss: 0.0990 - val_lower_loss: 0.0526 - val_upper_loss: 0.0464 - val_PICW: 25.4352\n",
      "Epoch 24/1000\n",
      "11/11 [==============================] - 4s 336ms/step - loss: 0.1068 - lower_loss: 0.0601 - upper_loss: 0.0468 - PICW: 23.1657 - val_loss: 0.0979 - val_lower_loss: 0.0513 - val_upper_loss: 0.0466 - val_PICW: 24.7614\n",
      "Epoch 25/1000\n",
      "11/11 [==============================] - 3s 316ms/step - loss: 0.1064 - lower_loss: 0.0601 - upper_loss: 0.0462 - PICW: 23.2059 - val_loss: 0.0979 - val_lower_loss: 0.0511 - val_upper_loss: 0.0467 - val_PICW: 23.2232\n",
      "Epoch 26/1000\n",
      "11/11 [==============================] - 3s 272ms/step - loss: 0.1054 - lower_loss: 0.0588 - upper_loss: 0.0466 - PICW: 22.9992 - val_loss: 0.0984 - val_lower_loss: 0.0517 - val_upper_loss: 0.0467 - val_PICW: 23.8973\n",
      "Epoch 27/1000\n",
      "11/11 [==============================] - 3s 284ms/step - loss: 0.1068 - lower_loss: 0.0602 - upper_loss: 0.0466 - PICW: 23.2758 - val_loss: 0.0974 - val_lower_loss: 0.0512 - val_upper_loss: 0.0462 - val_PICW: 24.0065\n",
      "Epoch 28/1000\n",
      "11/11 [==============================] - 3s 274ms/step - loss: 0.1058 - lower_loss: 0.0590 - upper_loss: 0.0468 - PICW: 23.1363 - val_loss: 0.0976 - val_lower_loss: 0.0509 - val_upper_loss: 0.0467 - val_PICW: 21.7827\n",
      "Epoch 29/1000\n",
      "11/11 [==============================] - 3s 275ms/step - loss: 0.1059 - lower_loss: 0.0592 - upper_loss: 0.0466 - PICW: 22.9338 - val_loss: 0.0981 - val_lower_loss: 0.0513 - val_upper_loss: 0.0468 - val_PICW: 24.0912\n"
     ]
    }
   ],
   "source": [
    "MAX_EPOCHS=1000\n",
    "history = model.fit(inputs,outputs,\n",
    "                    batch_size=300, \n",
    "                    epochs=MAX_EPOCHS, \n",
    "                    validation_data=(inputs_val,outputs_val), \n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "json.dump(history_dict, open(\"../data/training_PI/history_model.json\", 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"../data/sn_model_PI.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = np.load(\"../data/padded_x_val.npy\")[:,:,:]\n",
    "data_test, data_test_min_val, data_test_max_val = normalize(data_test)\n",
    "X_test, y_test = data_test[:,:-out_steps,:], data_test[:,-out_steps:, :]\n",
    "\n",
    "#Doing inference on Train data\n",
    "y_hat_train = model.predict(X_train)\n",
    "#Denormalizing train\n",
    "dX_train = denormalize(X_train, data_min_val,data_max_val)\n",
    "dy_hat_train = {}\n",
    "dy_hat_train[\"prediction\"] = denormalize(y_hat_train[\"prediction\"], data_min_val,data_max_val)\n",
    "for key in [\"upper\", \"lower\"]:\n",
    "    dy_hat_train[key] = denormalize(y_hat_train[key], data_min_val[:,1][:,np.newaxis],data_max_val[:,1][:,np.newaxis])\n",
    "dy_train = denormalize(y_train, data_min_val,data_max_val)\n",
    "\n",
    "# Doing inference on Test data\n",
    "y_hat = model.predict(X_test)\n",
    "# Denormalizing results\n",
    "dX_test = denormalize(X_test, data_test_min_val,data_test_max_val)\n",
    "dy_hat = {}\n",
    "dy_hat[\"prediction\"] = denormalize(y_hat[\"prediction\"],data_test_min_val,data_test_max_val) \n",
    "for key in [\"upper\", \"lower\"]:\n",
    "    dy_hat[key] = denormalize(y_hat[key],data_test_min_val[:,1][:,np.newaxis],data_test_max_val[:,1][:,np.newaxis])\n",
    "dy_test = denormalize(y_test,data_test_min_val,data_test_max_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3979c78643d542a591acc8efdc1a5def",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=397, description='sample', max=795), Output()), _dom_classes=('widget-in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(sample)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plot_data(x, y_real, y_hat, sample=0):\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.gca().invert_yaxis()\n",
    "    x_masked = np.ma.masked_where(x < 0, x)\n",
    "    plt.scatter(x_masked[sample,:,0], x_masked[sample,:,1], label=\"History\")\n",
    "    plt.scatter(y_real[sample,:,0], y_real[sample,:,1], label=\"Real\")\n",
    "    plt.scatter(y_hat[\"prediction\"][sample,:,0], y_hat[\"prediction\"][sample,:,1], label=\"Prediction\")\n",
    "    plt.fill_between(y_hat[\"prediction\"][sample,:,0], y_hat[\"lower\"][sample,:,0], y_hat[\"upper\"][sample,:,0], alpha=0.2)\n",
    "    plt.xlabel(\"Time $mjd-\\min(mjd)$\")\n",
    "    plt.ylabel(\"Mag\")\n",
    "    \n",
    "    \n",
    "\n",
    "f = lambda sample: plot_data(dX_test, dy_test, dy_hat,sample=sample)\n",
    "interact(f, sample=(0,len(dX_test)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (796 of 796) |######################| Elapsed Time: 0:05:31 ETA:  00:00:00"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import progressbar\n",
    "bar = progressbar.ProgressBar(max_value=len(X_test))\n",
    "os.makedirs(\"../data/plots_test_PI/\",exist_ok=True)\n",
    "\n",
    "x = dX_test\n",
    "y_real = dy_test\n",
    "y_hat = dy_hat\n",
    "bar.start()\n",
    "for sample in range(len(dX_test)):\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.gca().invert_yaxis()\n",
    "    x_masked = np.ma.masked_where(x < 0, x)\n",
    "    plt.scatter(x_masked[sample,:,0], x_masked[sample,:,1], label=\"History\")\n",
    "    plt.scatter(y_real[sample,:,0], y_real[sample,:,1], label=\"Real\")\n",
    "    plt.scatter(y_hat[\"prediction\"][sample,:,0], y_hat[\"prediction\"][sample,:,1], label=\"Prediction\")\n",
    "    plt.fill_between(y_hat[\"prediction\"][sample,:,0], y_hat[\"lower\"][sample,:,0], y_hat[\"upper\"][sample,:,0], alpha=0.2)\n",
    "    plt.xlabel(\"Time $mjd-\\min(mjd)$\")\n",
    "    plt.ylabel(\"Mag\")\n",
    "    plt.savefig(f\"../data/plots_test_PI/{str(sample).rjust(5,'0')}\")\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "    bar.update(sample+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
