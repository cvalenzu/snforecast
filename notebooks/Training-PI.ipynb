{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow import constant_initializer\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import shutil\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../code/\")\n",
    "from model import PhasedSNForecastProbabilisticIntervalModel\n",
    "out_steps = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data, min_val = None, max_val = None):\n",
    "    masked_data = np.ma.masked_where(data < 0, data)\n",
    "    min_val = np.tile([0,0,0], (len(data),1)) if min_val is None else min_val\n",
    "    max_val = np.tile([15, 23, 1],(len(data),1)) if max_val is None else max_val\n",
    "    \n",
    "    for i in range(masked_data.shape[1]):\n",
    "        masked_data.data[:,i,:] = (masked_data.data[:,i,:] - min_val)/(max_val-min_val)\n",
    "    \n",
    "    return_data = masked_data.data\n",
    "    return_data[masked_data.mask] = -1\n",
    "    return return_data, min_val, max_val\n",
    "    \n",
    "def denormalize(data, min_val, max_val):\n",
    "    masked_data = np.ma.masked_where(data < 0, data)\n",
    "    \n",
    "    for i in range(masked_data.shape[1]):\n",
    "        masked_data.data[:,i,:] = (masked_data.data[:,i,:] * (max_val-min_val))  +  min_val\n",
    "    \n",
    "    return_data = masked_data.data\n",
    "    return_data[masked_data.mask] = -1\n",
    "    return return_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"../data/padded_x_train.npy\")\n",
    "len_data = data.shape[1]\n",
    "X_train, y_train = data[:,:-out_steps,:],  data[:,-out_steps:,:]\n",
    "X_train, data_min_val, data_max_val = normalize(X_train)\n",
    "y_train, _, _ = normalize(y_train,min_val=data_min_val, max_val=data_max_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val = np.load(\"../data/padded_x_val.npy\")\n",
    "len_data = data_val.shape[1]\n",
    "X_val, y_val = data_val[:,:-out_steps,:],  data_val[:,-out_steps:,:]\n",
    "X_val, data_val_min_val, data_val_max_val = normalize(X_val)\n",
    "y_val, _, _ = normalize(y_val,min_val=data_val_min_val,max_val=data_val_max_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = X_train\n",
    "outputs = y_train\n",
    "inputs_val = X_val\n",
    "outputs_val = y_val\n",
    "\n",
    "outputs = {}\n",
    "outputs_val = {}\n",
    "\n",
    "outputs[\"prediction\"] = y_train\n",
    "outputs_val[\"prediction\"] = y_val\n",
    "\n",
    "for interval in [\"upper\", \"lower\"]:\n",
    "    outputs[interval] = np.expand_dims(y_train[:,:,1],axis=-1)\n",
    "    outputs_val[interval] = np.expand_dims(y_val[:,:,1],axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveData(tf.keras.callbacks.Callback):\n",
    "    def __init__(self,logdir, keys,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.file_writer = tf.summary.create_file_writer(logdir + \"/metrics\")\n",
    "        self.file_writer.set_as_default()\n",
    "        self.keys = keys\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        for key in self.keys:\n",
    "            tf.summary.scalar(key, data=logs.get(key), step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "#Early stops\n",
    "early_stop = tf.keras.callbacks.EarlyStopping( monitor='val_loss', min_delta=1e-10, patience=10)\n",
    "\n",
    "#Tensorboard\n",
    "logdir = \"../data/training/logs/PI\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(logdir)\n",
    "saver = SaveData(logdir, [\"PICW\"])\n",
    "shutil.rmtree(\"../data/training/logs/PI\",ignore_errors=True)\n",
    "\n",
    "\n",
    "#Checkpoint\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\"../data/training_PI/model_checkpoints/checkpoint\", monitor='val_loss', verbose=0, save_best_only=True)\n",
    "\n",
    "callbacks = [tensorboard,checkpoint, early_stop, saver]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 834us/step - loss: 0.2766\n"
     ]
    }
   ],
   "source": [
    "#Loading and preparing model\n",
    "from model import PhasedSNForecastModel\n",
    "base_model = PhasedSNForecastModel(units=64, out_steps=out_steps,features = 3)\n",
    "base_model.compile(optimizer=\"rmsprop\", loss=\"mse\")\n",
    "_ = base_model.fit(X_train[:2], y_train[:2])\n",
    "\n",
    "\n",
    "base_model.load_weights(\"../data/sn_model_small.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_addons.utils.keras_utils import LossFunctionWrapper\n",
    "from tensorflow_addons.utils.types import TensorLike, FloatTensorLike\n",
    "from typeguard import typechecked\n",
    "\n",
    "@tf.function\n",
    "def custom_pinball_loss(y_true: TensorLike, y_pred: TensorLike, tau: FloatTensorLike = 0.5) -> tf.Tensor:\n",
    "    y_pred = tf.convert_to_tensor(y_pred)\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "    \n",
    "    \n",
    "    # Broadcast the pinball slope along the batch dimension\n",
    "    tau = tf.expand_dims(tf.cast(tau, y_pred.dtype), 0)\n",
    "    one = tf.cast(1, tau.dtype)\n",
    "\n",
    "    pinball = tf.where(y_pred > y_true, tau * (y_pred - y_true), (1-tau) * (y_true-y_pred) )\n",
    "    return tf.reduce_mean(pinball, axis=-1)\n",
    "\n",
    "class CustomPinballLoss(LossFunctionWrapper):\n",
    "    @typechecked\n",
    "    def __init__(\n",
    "        self,\n",
    "        tau: FloatTensorLike = 0.5,\n",
    "        reduction: str = tf.keras.losses.Reduction.AUTO,\n",
    "        name: str = \"custom_pinball_loss\",\n",
    "    ):\n",
    "        super().__init__(custom_pinball_loss, reduction=reduction, name=name, tau=tau)\n",
    "        \n",
    "        \n",
    "@tf.function\n",
    "def inverse_pinball_loss(y_true: TensorLike, y_pred: TensorLike, tau: FloatTensorLike = 0.5) -> tf.Tensor:\n",
    "    y_pred = tf.convert_to_tensor(y_pred)\n",
    "    y_true = tf.cast(y_true, y_pred.dtype)\n",
    "    \n",
    "    \n",
    "    # Broadcast the pinball slope along the batch dimension\n",
    "    tau = tf.expand_dims(tf.cast(tau, y_pred.dtype), 0)\n",
    "    one = tf.cast(1, tau.dtype)\n",
    "\n",
    "    pinball = tf.where(y_pred > y_true, (1-tau) * (y_pred - y_true), tau * (y_true-y_pred) )\n",
    "    return tf.reduce_mean(pinball, axis=-1)   \n",
    "\n",
    "class InversePinballLoss(LossFunctionWrapper):\n",
    "    @typechecked\n",
    "    def __init__(\n",
    "        self,\n",
    "        tau: FloatTensorLike = 0.5,\n",
    "        reduction: str = tf.keras.losses.Reduction.AUTO,\n",
    "        name: str = \"inverse_pinball_loss\",\n",
    "    ):\n",
    "        super().__init__(inverse_pinball_loss, reduction=reduction, name=name, tau=tau)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PhasedSNForecastProbabilisticIntervalModel(units=300, out_steps=out_steps, model = base_model, dropout=0.0)\n",
    "model.rnn.trainable = False\n",
    "model.denses.trainable = False\n",
    "model.cells.trainable = False\n",
    "\n",
    "alpha = 0.30\n",
    "losses = {\n",
    "    \"prediction\": None,\n",
    "    \"lower\": CustomPinballLoss(tau=(alpha/2), reduction=tf.keras.losses.Reduction.NONE),\n",
    "    \"upper\": CustomPinballLoss(tau=1-(alpha/2), reduction=tf.keras.losses.Reduction.NONE)\n",
    "}\n",
    "model.compile(optimizer=\"rmsprop\", loss=losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      " 2/30 [=>............................] - ETA: 3s - loss: 0.1186 - lower_loss: 0.0361 - upper_loss: 0.0235 - PICW: 0.0590WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0441s vs `on_train_batch_end` time: 0.1825s). Check your callbacks.\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 0.2903 - lower_loss: 0.0690 - upper_loss: 0.0505 - PICW: 0.1708 - val_loss: 0.3393 - val_lower_loss: 0.0951 - val_upper_loss: 0.0618 - val_PICW: 0.1824\n",
      "Epoch 2/1000\n",
      "30/30 [==============================] - 2s 70ms/step - loss: 0.2186 - lower_loss: 0.0609 - upper_loss: 0.0416 - PICW: 0.1156 - val_loss: 0.0620 - val_lower_loss: 0.0282 - val_upper_loss: 0.0149 - val_PICW: 0.0189\n",
      "Epoch 3/1000\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 0.2781 - lower_loss: 0.0700 - upper_loss: 0.0534 - PICW: 0.1542 - val_loss: 0.0953 - val_lower_loss: 0.0376 - val_upper_loss: 0.0170 - val_PICW: 0.0407\n",
      "Epoch 4/1000\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 0.1914 - lower_loss: 0.0531 - upper_loss: 0.0380 - PICW: 0.1000 - val_loss: 0.0564 - val_lower_loss: 0.0260 - val_upper_loss: 0.0149 - val_PICW: 0.0154\n",
      "Epoch 5/1000\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 0.3146 - lower_loss: 0.0737 - upper_loss: 0.0576 - PICW: 0.1831 - val_loss: 0.2410 - val_lower_loss: 0.0728 - val_upper_loss: 0.0399 - val_PICW: 0.1283\n",
      "Epoch 6/1000\n",
      "30/30 [==============================] - 2s 70ms/step - loss: 0.2020 - lower_loss: 0.0568 - upper_loss: 0.0374 - PICW: 0.1080 - val_loss: 0.2501 - val_lower_loss: 0.0711 - val_upper_loss: 0.0459 - val_PICW: 0.1331\n",
      "Epoch 7/1000\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 0.2748 - lower_loss: 0.0679 - upper_loss: 0.0520 - PICW: 0.1551 - val_loss: 0.3466 - val_lower_loss: 0.0938 - val_upper_loss: 0.0661 - val_PICW: 0.1868\n",
      "Epoch 8/1000\n",
      "30/30 [==============================] - 2s 70ms/step - loss: 0.1915 - lower_loss: 0.0538 - upper_loss: 0.0389 - PICW: 0.0998 - val_loss: 0.1595 - val_lower_loss: 0.0475 - val_upper_loss: 0.0305 - val_PICW: 0.0816\n",
      "Epoch 9/1000\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 0.2772 - lower_loss: 0.0662 - upper_loss: 0.0540 - PICW: 0.1570 - val_loss: 0.2931 - val_lower_loss: 0.0814 - val_upper_loss: 0.0543 - val_PICW: 0.1573\n",
      "Epoch 10/1000\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 0.2207 - lower_loss: 0.0600 - upper_loss: 0.0430 - PICW: 0.1178 - val_loss: 0.2603 - val_lower_loss: 0.0732 - val_upper_loss: 0.0475 - val_PICW: 0.1395\n",
      "Epoch 11/1000\n",
      "30/30 [==============================] - 2s 72ms/step - loss: 0.2140 - lower_loss: 0.0557 - upper_loss: 0.0418 - PICW: 0.1169 - val_loss: 0.4206 - val_lower_loss: 0.1074 - val_upper_loss: 0.0860 - val_PICW: 0.2271\n",
      "Epoch 12/1000\n",
      "30/30 [==============================] - 5s 152ms/step - loss: 0.2232 - lower_loss: 0.0589 - upper_loss: 0.0450 - PICW: 0.1193 - val_loss: 0.2238 - val_lower_loss: 0.0644 - val_upper_loss: 0.0411 - val_PICW: 0.1183\n",
      "Epoch 13/1000\n",
      "30/30 [==============================] - 4s 127ms/step - loss: 0.2352 - lower_loss: 0.0610 - upper_loss: 0.0459 - PICW: 0.1284 - val_loss: 0.3007 - val_lower_loss: 0.0832 - val_upper_loss: 0.0557 - val_PICW: 0.1618\n",
      "Epoch 14/1000\n",
      "30/30 [==============================] - 4s 127ms/step - loss: 0.2393 - lower_loss: 0.0634 - upper_loss: 0.0455 - PICW: 0.1301 - val_loss: 0.1262 - val_lower_loss: 0.0452 - val_upper_loss: 0.0193 - val_PICW: 0.0618\n"
     ]
    }
   ],
   "source": [
    "MAX_EPOCHS=1000\n",
    "history = model.fit(inputs,outputs,\n",
    "                    batch_size=150, \n",
    "                    epochs=MAX_EPOCHS, \n",
    "                    validation_data=(inputs_val,outputs_val), \n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "json.dump(history_dict, open(\"../data/training_PI/history_model.json\", 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"../data/sn_model_PI_small.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = np.load(\"../data/padded_x_test.npy\")[:,:,:]\n",
    "# data_test, data_test_min_val, data_test_max_val = normalize(data_test)\n",
    "X_test, y_test = data_test[:,:-out_steps,:], data_test[:,-out_steps:, :]\n",
    "X_test, data_test_min_val, data_test_max_val = normalize(X_test)\n",
    "y_test, _, _ = normalize(y_test,min_val=data_test_min_val, max_val=data_test_max_val)\n",
    "\n",
    "#Doing inference on Train data\n",
    "y_hat_train = model.predict(X_train)\n",
    "#Denormalizing train\n",
    "dX_train = denormalize(X_train, data_min_val,data_max_val)\n",
    "dy_hat_train = {}\n",
    "dy_hat_train[\"prediction\"] = denormalize(y_hat_train[\"prediction\"], data_min_val,data_max_val)\n",
    "for key in [\"upper\", \"lower\"]:\n",
    "    dy_hat_train[key] = denormalize(y_hat_train[key], data_min_val[:,1][:,np.newaxis],data_max_val[:,1][:,np.newaxis])\n",
    "dy_train = denormalize(y_train, data_min_val,data_max_val)\n",
    "\n",
    "# Doing inference on Test data\n",
    "y_hat = model.predict(X_test)\n",
    "# Denormalizing results\n",
    "dX_test = denormalize(X_test, data_test_min_val,data_test_max_val)\n",
    "dy_hat = {}\n",
    "dy_hat[\"prediction\"] = denormalize(y_hat[\"prediction\"],data_test_min_val,data_test_max_val) \n",
    "for key in [\"upper\", \"lower\"]:\n",
    "    dy_hat[key] = denormalize(y_hat[key],data_test_min_val[:,1][:,np.newaxis],data_test_max_val[:,1][:,np.newaxis])\n",
    "dy_test = denormalize(y_test,data_test_min_val,data_test_max_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e654f75d9e6c4f95b5f4465cebaff7ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=728, description='sample', max=1456), Output()), _dom_classes=('widget-i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(sample)>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "def plot_data(x, y_real, y_hat, sample=0, save=False, path=\"\"):\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "    xx = x[sample]\n",
    "    x_slice = np.where(~(xx[:,1] < 0) )[0]\n",
    "    unpadded_xx = xx[x_slice,:]\n",
    "    xx_time = unpadded_xx\n",
    "    xx_time[:,0] = np.cumsum(xx_time[:,0]) \n",
    "    last_time = xx_time[:,0][-1]\n",
    "\n",
    "    y_real_sample = y_real[sample]\n",
    "    y_real_sample[:,0] = np.cumsum(y_real_sample[:,0]) + last_time\n",
    "\n",
    "    y_hat_sample = y_hat[\"prediction\"][sample]\n",
    "    y_hat_sample[:,0] = np.cumsum(y_hat_sample[:,0]) + last_time\n",
    "\n",
    "    plt.scatter(xx_time[:,0], xx_time[:,1], label=\"History\")\n",
    "    plt.scatter(y_real_sample[:,0], y_real_sample[:,1], label=\"Real\")\n",
    "    plt.scatter(y_hat_sample[:,0], y_hat_sample[:,1], label=\"Prediction\")\n",
    "    plt.fill_between(y_hat_sample[:,0], y_hat[\"lower\"][sample,:,0], y_hat[\"upper\"][sample,:,0], alpha=0.2, label=\"Pinball Loss Q(0.15,0.85)\")\n",
    "    plt.xlabel(\"Time $mjd-\\min(mjd)$\")\n",
    "    plt.ylabel(\"Mag\")\n",
    "    plt.legend()\n",
    "    if save:\n",
    "        plt.savefig(os.path.join(path, f\"{str(sample).rjust(5,'0')}.png\"))\n",
    "        plt.cla()\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "f = lambda sample: plot_data(dX_test, dy_test, dy_hat,sample=sample)\n",
    "interact(f, sample=(0,len(dX_test)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import progressbar\n",
    "# bar = progressbar.ProgressBar(max_value=len(X_test))\n",
    "# os.makedirs(\"../data/plots_test_PI/\",exist_ok=True)\n",
    "\n",
    "# x = dX_test\n",
    "# y_real = dy_test\n",
    "# y_hat = dy_hat\n",
    "# bar.start()\n",
    "# for sample in range(len(dX_test)):\n",
    "#     plt.figure(figsize=(12,6))\n",
    "#     plt.gca().invert_yaxis()\n",
    "#     x_masked = np.ma.masked_where(x < 0, x)\n",
    "#     plt.scatter(x_masked[sample,:,0], x_masked[sample,:,1], label=\"History\")\n",
    "#     plt.scatter(y_real[sample,:,0], y_real[sample,:,1], label=\"Real\")\n",
    "#     plt.scatter(y_hat[\"prediction\"][sample,:,0], y_hat[\"prediction\"][sample,:,1], label=\"Prediction\")\n",
    "#     plt.fill_between(y_hat[\"prediction\"][sample,:,0], y_hat[\"lower\"][sample,:,0], y_hat[\"upper\"][sample,:,0], alpha=0.2)\n",
    "#     plt.xlabel(\"Time $mjd-\\min(mjd)$\")\n",
    "#     plt.ylabel(\"Mag\")\n",
    "#     plt.savefig(f\"../data/plots_test_PI/{str(sample).rjust(5,'0')}\")\n",
    "#     plt.clf()\n",
    "#     plt.cla()\n",
    "#     plt.close()\n",
    "#     bar.update(sample+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
