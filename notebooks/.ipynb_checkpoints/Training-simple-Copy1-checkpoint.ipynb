{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import constant_initializer\n",
    "\n",
    "def _exponential_initializer(min, max, dtype=None):\n",
    "    def in_func(shape, dtype=dtype):\n",
    "        initializer = tf.random_uniform_initializer(\n",
    "                        tf.math.log(1.0),\n",
    "                        tf.math.log(100.0)\n",
    "                        )\n",
    "        return tf.math.exp(initializer(shape))\n",
    "    return in_func\n",
    "\n",
    "class PhasedLSTM(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 units,\n",
    "                 leak_rate=0.001,\n",
    "                 ratio_on=0.1,\n",
    "                 period_init_min=0.0,\n",
    "                 period_init_max=1000.0,\n",
    "                 rec_activation = tf.math.sigmoid,\n",
    "                 out_activation = tf.math.tanh,\n",
    "                 name='plstm',\n",
    "                 **kwargs):\n",
    "        super(PhasedLSTM, self).__init__(name=name)\n",
    "        \n",
    "        self.state_size = [units,units] #This change\n",
    "        self.output_size = units        #This change\n",
    "        \n",
    "        self.units = units\n",
    "        self._leak = leak_rate\n",
    "        self._ratio_on = ratio_on\n",
    "        self._rec_activation = rec_activation\n",
    "        self._out_activation = out_activation\n",
    "        self.period_init_min = period_init_min\n",
    "        self.period_init_max = period_init_max\n",
    "        \n",
    "        self.cell = tf.keras.layers.LSTMCell(units, **kwargs)\n",
    "\n",
    "    def _get_cycle_ratio(self, time, phase, period):\n",
    "        \"\"\"Compute the cycle ratio in the dtype of the time.\"\"\"\n",
    "        phase_casted = tf.cast(phase, dtype=time.dtype)\n",
    "        period_casted = tf.cast(period, dtype=time.dtype)\n",
    "        time = tf.reshape(time, [tf.shape(time)[0],1]) #This change\n",
    "        shifted_time = time - phase_casted\n",
    "        cycle_ratio = (shifted_time%period_casted) / period_casted\n",
    "        return tf.cast(cycle_ratio, dtype=tf.float32)        \n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.period = self.add_weight(\n",
    "                        name=\"period\",\n",
    "                        shape=[self.units],\n",
    "                        initializer=_exponential_initializer(\n",
    "                                            self.period_init_min,\n",
    "                                            self.period_init_max),\n",
    "                        trainable=True)\n",
    "\n",
    "        self.phase = self.add_weight(name=\"phase\",\n",
    "                                     shape=[self.units],\n",
    "                                     initializer=tf.random_uniform_initializer(\n",
    "                                                         0.0,\n",
    "                                                         self.period),\n",
    "                                     trainable=True)\n",
    "        self.ratio_on = self.add_weight(name=\"ratio_on\",\n",
    "                                        shape=[self.units],\n",
    "                                        initializer=constant_initializer(self._ratio_on),\n",
    "                                        trainable=True)\n",
    "\n",
    "    def call(self, input, states):\n",
    "        inputs, times = input, input[:,0] #This change\n",
    "\n",
    "        # =================================\n",
    "        # CANDIDATE CELL AND HIDDEN STATE\n",
    "        # =================================\n",
    "        prev_hs, prev_cs = states\n",
    "        output, (hs, cs) = self.cell(inputs, states)\n",
    "\n",
    "        # =================================\n",
    "        # TIME GATE\n",
    "        # =================================\n",
    "        cycle_ratio = self._get_cycle_ratio(times, self.phase, self.period)\n",
    "\n",
    "        k_up = 2 * cycle_ratio / self.ratio_on\n",
    "        k_down = 2 - k_up\n",
    "        k_closed = self._leak * cycle_ratio\n",
    "\n",
    "        k = tf.where(cycle_ratio < self.ratio_on, k_down, k_closed)\n",
    "        k = tf.where(cycle_ratio < 0.5 * self.ratio_on, k_up, k)\n",
    "\n",
    "        # =================================\n",
    "        # UPDATE STATE USING TIME GATE VALUES\n",
    "        # =================================\n",
    "        new_h = k * hs + (1 - k) * prev_hs\n",
    "        new_c = k * cs + (1 - k) * prev_cs\n",
    "\n",
    "        return new_h, (new_h, new_c)\n",
    "\n",
    "    \n",
    "    \n",
    "class PhasedSNForecastModel(tf.keras.Model):\n",
    "    def __init__(self, units, out_steps):\n",
    "        super().__init__()\n",
    "        self.out_steps = out_steps\n",
    "        self.units = units\n",
    "        self.d1 = tf.keras.layers.Dropout(0.2)\n",
    "        self.concat = tf.keras.layers.Concatenate()\n",
    "        self.mask = tf.keras.layers.Masking(mask_value=-1.0)\n",
    "        self._init_dense()\n",
    "        self._init_recurrent()\n",
    "\n",
    "    def normalization(self,inputs):\n",
    "        max_all = tf.keras.backend.max(inputs)\n",
    "        max_val = tf.keras.backend.max(inputs,axis=1)\n",
    "        min_val = tf.keras.backend.min(tf.where(inputs > -1, inputs, max_all*tf.ones_like(inputs)),axis=1)\n",
    "\n",
    "        stacked_min_val = tf.stack([min_val for i in range(inputs.shape[1])])\n",
    "        stacked_min_val = tf.transpose(stacked_min_val, [1, 0, 2])\n",
    "        stacked_max_val = tf.stack([max_val for i in range(inputs.shape[1])])\n",
    "        stacked_max_val = tf.transpose(stacked_max_val, [1, 0, 2])\n",
    "\n",
    "        inputs = (inputs - stacked_min_val) / (stacked_max_val-stacked_min_val)\n",
    "        return min_val, max_val, inputs\n",
    "\n",
    "    def denormalize(self,inputs, min_val, max_val):\n",
    "        stacked_min_val = tf.stack([min_val for i in range(self.out_steps)])\n",
    "        stacked_min_val = tf.transpose(stacked_min_val, [1, 0, 2])\n",
    "        stacked_max_val = tf.stack([max_val for i in range(self.out_steps)])\n",
    "        stacked_max_val = tf.transpose(stacked_max_val, [1, 0, 2])\n",
    "\n",
    "        inputs = inputs * (stacked_max_val-stacked_min_val) + stacked_min_val\n",
    "        return inputs\n",
    "\n",
    "    def fowardpass(self, cells, states, denses, x, training=None):\n",
    "        for i,cell in enumerate(cells):\n",
    "            x, states[i] = cell(x, states=states[i],\n",
    "                      training=training)\n",
    "        for layer in denses:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x, states\n",
    "\n",
    "    def _warmup(self,rnn, denses, inputs):\n",
    "        x, *state = rnn(inputs)\n",
    "        for layer in denses:\n",
    "            x = layer(x)\n",
    "        return x, state\n",
    "\n",
    "    def warmups(self,inputs):\n",
    "        inputs = self.mask(inputs)\n",
    "        prediction,states = self._warmup(self.rnn,self.denses,inputs)\n",
    "        return prediction,states\n",
    "\n",
    "    def _init_recurrent(self):\n",
    "        cell1 = PhasedLSTM(self.units, activation=\"sigmoid\", dropout=0.2)\n",
    "        self.cells = [cell1]\n",
    "        self.rnn = tf.keras.layers.RNN(self.cells, return_state=True)\n",
    "\n",
    "    def _init_dense(self):\n",
    "        dense1 = tf.keras.layers.Dense(self.units//2, activation=\"sigmoid\")\n",
    "        dense2 = tf.keras.layers.Dense(self.units//4, activation=\"sigmoid\")\n",
    "        dense3 = tf.keras.layers.Dense(self.units//8, activation=\"sigmoid\")\n",
    "\n",
    "        out = tf.keras.layers.Dense(2, activation=\"linear\")\n",
    "        self.denses = [dense1,self.d1,dense2, self.d1,dense3, self.d1,out]\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        min_val, max_val, inputs =  self.normalization(inputs)\n",
    "        #Creating empty tensors for predictions\n",
    "        predictions = []\n",
    "        prediction, states = self.warmups(inputs)\n",
    "\n",
    "        #Saving first predictions\n",
    "        predictions.append(prediction)\n",
    "\n",
    "        for n in range(1, self.out_steps):\n",
    "            prediction, states = self.fowardpass(self.cells, states, self.denses, prediction, training)\n",
    "            predictions.append(prediction)\n",
    "\n",
    "        #Stacking predictions\n",
    "        predictions = tf.stack(predictions)\n",
    "        predictions = tf.transpose(predictions, [1, 0, 2])\n",
    "        predictions = self.denormalize(predictions, min_val, max_val)\n",
    "        return predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys \n",
    "# sys.path.append(\"../code/\")\n",
    "# from model import SNForecastModel\n",
    "# from phased_model import PhasedSNForecastModel\n",
    "out_steps = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_true,y_pred,output_steps=10):\n",
    "    maes = []\n",
    "    for i in range(out_steps):\n",
    "        maes.append(tf.keras.losses.MAE(y_true[:,i,:],y_pred[:,i,:]))\n",
    "    maes = tf.stack(maes)\n",
    "    return tf.reduce_mean(maes,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PhasedSNForecastModel(units=64, out_steps=out_steps)\n",
    "losses = [tf.keras.losses.MeanAbsoluteError(reduction=tf.keras.losses.Reduction.NONE)]#\"mse\"\n",
    "model.compile(optimizer=\"adam\", loss=losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"../data/padded_x_train.npy\")\n",
    "\n",
    "\n",
    "len_data = data.shape[1]\n",
    "X_train, y_train = data[:,:-out_steps,:2],  data[:,-out_steps:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val = np.load(\"../data/padded_x_val.npy\")\n",
    "len_data = data_val.shape[1]\n",
    "\n",
    "X_val, y_val = data_val[:,:-out_steps,:2],  data_val[:,-out_steps:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = X_train\n",
    "outputs = y_train\n",
    "inputs_val = X_val\n",
    "outputs_val = y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Early stops\n",
    "early_stop = tf.keras.callbacks.EarlyStopping( monitor='val_loss', min_delta=1e-8, patience=5)\n",
    "\n",
    "#Tensorboard\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(\"../data/training/logs\")\n",
    "shutil.rmtree(\"../data/training/logs\",ignore_errors=True)\n",
    "#Checkpoint\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\"../data/training/model_checkpoints/checkpoint\", monitor='val_loss', verbose=0, save_best_only=True)\n",
    "\n",
    "callbacks = [tensorboard,checkpoint, early_stop] # mag_early_stop,fid_early_stop,dt_early_stop,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "11/11 [==============================] - 2s 200ms/step - loss: 17.3382 - val_loss: 17.0812\n",
      "Epoch 2/1000\n",
      "11/11 [==============================] - 1s 129ms/step - loss: 14.6734 - val_loss: 13.6895\n",
      "Epoch 3/1000\n",
      "11/11 [==============================] - 2s 149ms/step - loss: 12.2474 - val_loss: 10.4813\n",
      "Epoch 4/1000\n",
      "11/11 [==============================] - 2s 148ms/step - loss: 10.5895 - val_loss: 7.7389\n",
      "Epoch 5/1000\n",
      "11/11 [==============================] - 2s 138ms/step - loss: 9.8141 - val_loss: 6.6677\n",
      "Epoch 6/1000\n",
      "11/11 [==============================] - 2s 139ms/step - loss: 9.4178 - val_loss: 6.5150\n",
      "Epoch 7/1000\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 9.2199 - val_loss: 6.5308\n",
      "Epoch 8/1000\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 9.1099 - val_loss: 6.5550\n",
      "Epoch 9/1000\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 9.1199 - val_loss: 6.5591\n",
      "Epoch 10/1000\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 9.0574 - val_loss: 6.5404\n",
      "Epoch 11/1000\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 8.9348 - val_loss: 6.5399\n",
      "Epoch 12/1000\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 8.9005 - val_loss: 6.5174\n",
      "Epoch 13/1000\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 8.8245 - val_loss: 6.5116\n",
      "Epoch 14/1000\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 8.8172 - val_loss: 6.5072\n",
      "Epoch 15/1000\n",
      "11/11 [==============================] - 2s 193ms/step - loss: 8.7400 - val_loss: 6.4784\n",
      "Epoch 16/1000\n",
      "11/11 [==============================] - 2s 176ms/step - loss: 8.6171 - val_loss: 6.4571\n",
      "Epoch 17/1000\n",
      "11/11 [==============================] - 3s 265ms/step - loss: 8.6303 - val_loss: 6.4569\n",
      "Epoch 18/1000\n",
      "11/11 [==============================] - 4s 356ms/step - loss: 8.5928 - val_loss: 6.4354\n",
      "Epoch 19/1000\n",
      "11/11 [==============================] - 4s 354ms/step - loss: 8.4934 - val_loss: 6.4193\n",
      "Epoch 20/1000\n",
      "11/11 [==============================] - 4s 341ms/step - loss: 8.5005 - val_loss: 6.4037\n",
      "Epoch 21/1000\n",
      "11/11 [==============================] - 3s 277ms/step - loss: 8.4040 - val_loss: 6.4036\n",
      "Epoch 22/1000\n",
      "11/11 [==============================] - 3s 286ms/step - loss: 8.3665 - val_loss: 6.4031\n",
      "Epoch 23/1000\n",
      "11/11 [==============================] - 4s 345ms/step - loss: 8.3530 - val_loss: 6.3912\n",
      "Epoch 24/1000\n",
      "11/11 [==============================] - 4s 332ms/step - loss: 8.3290 - val_loss: 6.3935\n",
      "Epoch 25/1000\n",
      "11/11 [==============================] - 3s 298ms/step - loss: 8.2574 - val_loss: 6.3903\n",
      "Epoch 26/1000\n",
      "11/11 [==============================] - 4s 322ms/step - loss: 8.2158 - val_loss: 6.3828\n",
      "Epoch 27/1000\n",
      "11/11 [==============================] - 3s 266ms/step - loss: 8.2275 - val_loss: 6.3408\n",
      "Epoch 28/1000\n",
      "11/11 [==============================] - 2s 207ms/step - loss: 8.1304 - val_loss: 6.3458\n",
      "Epoch 29/1000\n",
      "11/11 [==============================] - 2s 210ms/step - loss: 8.1036 - val_loss: 6.3557\n",
      "Epoch 30/1000\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 8.1323 - val_loss: 6.2750\n",
      "Epoch 31/1000\n",
      "11/11 [==============================] - 2s 203ms/step - loss: 8.0109 - val_loss: 6.2358\n",
      "Epoch 32/1000\n",
      "11/11 [==============================] - 3s 227ms/step - loss: 8.0091 - val_loss: 6.2121\n",
      "Epoch 33/1000\n",
      "11/11 [==============================] - 3s 260ms/step - loss: 7.9760 - val_loss: 6.1616\n",
      "Epoch 34/1000\n",
      "11/11 [==============================] - 3s 232ms/step - loss: 7.9098 - val_loss: 6.1615\n",
      "Epoch 35/1000\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 7.8245 - val_loss: 6.1039\n",
      "Epoch 36/1000\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 7.8108 - val_loss: 5.9604\n",
      "Epoch 37/1000\n",
      "11/11 [==============================] - 2s 193ms/step - loss: 7.7227 - val_loss: 5.9541\n",
      "Epoch 38/1000\n",
      "11/11 [==============================] - 2s 175ms/step - loss: 7.6256 - val_loss: 5.7681\n",
      "Epoch 39/1000\n",
      "11/11 [==============================] - 2s 197ms/step - loss: 7.6075 - val_loss: 5.6910\n",
      "Epoch 40/1000\n",
      "11/11 [==============================] - 2s 187ms/step - loss: 7.5224 - val_loss: 5.6221\n",
      "Epoch 41/1000\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 7.4472 - val_loss: 5.7330\n",
      "Epoch 42/1000\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 7.4112 - val_loss: 5.4257\n",
      "Epoch 43/1000\n",
      "11/11 [==============================] - 2s 193ms/step - loss: 7.3748 - val_loss: 5.6001\n",
      "Epoch 44/1000\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 7.3192 - val_loss: 5.5763\n",
      "Epoch 45/1000\n",
      "11/11 [==============================] - 2s 174ms/step - loss: 7.2722 - val_loss: 5.3293\n",
      "Epoch 46/1000\n",
      "11/11 [==============================] - 2s 183ms/step - loss: 7.1616 - val_loss: 5.2949\n",
      "Epoch 47/1000\n",
      "11/11 [==============================] - 2s 193ms/step - loss: 7.1875 - val_loss: 5.3882\n",
      "Epoch 48/1000\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 7.0769 - val_loss: 5.3133\n",
      "Epoch 49/1000\n",
      "11/11 [==============================] - 2s 183ms/step - loss: 7.0294 - val_loss: 5.2058\n",
      "Epoch 50/1000\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 7.0630 - val_loss: 5.2322\n",
      "Epoch 51/1000\n",
      "11/11 [==============================] - 2s 177ms/step - loss: 7.0118 - val_loss: 5.1047\n",
      "Epoch 52/1000\n",
      "11/11 [==============================] - 2s 195ms/step - loss: 6.9146 - val_loss: 5.1082\n",
      "Epoch 53/1000\n",
      "11/11 [==============================] - 2s 195ms/step - loss: 6.9088 - val_loss: 5.1750\n",
      "Epoch 54/1000\n",
      "11/11 [==============================] - 2s 176ms/step - loss: 6.9311 - val_loss: 5.0304\n",
      "Epoch 55/1000\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 6.8609 - val_loss: 4.9919\n",
      "Epoch 56/1000\n",
      "11/11 [==============================] - 2s 193ms/step - loss: 6.8123 - val_loss: 5.0775\n",
      "Epoch 57/1000\n",
      "11/11 [==============================] - 2s 179ms/step - loss: 6.7780 - val_loss: 5.1292\n",
      "Epoch 58/1000\n",
      "11/11 [==============================] - 2s 186ms/step - loss: 6.7926 - val_loss: 5.3230\n",
      "Epoch 59/1000\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 6.7799 - val_loss: 4.9858\n",
      "Epoch 60/1000\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 6.7769 - val_loss: 5.0448\n",
      "Epoch 61/1000\n",
      "11/11 [==============================] - 2s 188ms/step - loss: 6.7138 - val_loss: 5.0151\n",
      "Epoch 62/1000\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 6.7103 - val_loss: 4.9895\n",
      "Epoch 63/1000\n",
      "11/11 [==============================] - 2s 193ms/step - loss: 6.6837 - val_loss: 4.9236\n",
      "Epoch 64/1000\n",
      "11/11 [==============================] - 2s 176ms/step - loss: 6.6768 - val_loss: 4.9371\n",
      "Epoch 65/1000\n",
      "11/11 [==============================] - 2s 190ms/step - loss: 6.5597 - val_loss: 5.0317\n",
      "Epoch 66/1000\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 6.6183 - val_loss: 4.9412\n",
      "Epoch 67/1000\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 6.5743 - val_loss: 4.8609\n",
      "Epoch 68/1000\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 6.5091 - val_loss: 4.9140\n",
      "Epoch 69/1000\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 6.5573 - val_loss: 4.8257\n",
      "Epoch 70/1000\n",
      "11/11 [==============================] - 2s 170ms/step - loss: 6.4520 - val_loss: 4.8358\n",
      "Epoch 71/1000\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 6.4847 - val_loss: 4.9312\n",
      "Epoch 72/1000\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 6.4943 - val_loss: 4.8057\n",
      "Epoch 73/1000\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 6.4294 - val_loss: 4.8781\n",
      "Epoch 74/1000\n",
      "11/11 [==============================] - 2s 210ms/step - loss: 6.3781 - val_loss: 4.8522\n",
      "Epoch 75/1000\n",
      "11/11 [==============================] - 2s 199ms/step - loss: 6.3362 - val_loss: 4.7639\n",
      "Epoch 76/1000\n",
      "11/11 [==============================] - 2s 181ms/step - loss: 6.3191 - val_loss: 4.7710\n",
      "Epoch 77/1000\n",
      "11/11 [==============================] - 2s 195ms/step - loss: 6.3620 - val_loss: 4.7447\n",
      "Epoch 78/1000\n",
      "11/11 [==============================] - 2s 188ms/step - loss: 6.2222 - val_loss: 4.7794\n",
      "Epoch 79/1000\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 6.2374 - val_loss: 4.7013\n",
      "Epoch 80/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 2s 177ms/step - loss: 6.2415 - val_loss: 4.7527\n",
      "Epoch 81/1000\n",
      "11/11 [==============================] - 2s 189ms/step - loss: 6.1574 - val_loss: 4.6823\n",
      "Epoch 82/1000\n",
      "11/11 [==============================] - 2s 186ms/step - loss: 6.1787 - val_loss: 4.6894\n",
      "Epoch 83/1000\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 6.1562 - val_loss: 4.7232\n",
      "Epoch 84/1000\n",
      "11/11 [==============================] - 2s 183ms/step - loss: 6.1410 - val_loss: 4.6793\n",
      "Epoch 85/1000\n",
      "11/11 [==============================] - 2s 188ms/step - loss: 6.1099 - val_loss: 4.7394\n",
      "Epoch 86/1000\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 6.0749 - val_loss: 4.6911\n",
      "Epoch 87/1000\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 6.0283 - val_loss: 4.7995\n",
      "Epoch 88/1000\n",
      "11/11 [==============================] - 2s 193ms/step - loss: 6.0395 - val_loss: 4.6983\n",
      "Epoch 89/1000\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 6.0069 - val_loss: 4.6775\n",
      "Epoch 90/1000\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 5.9511 - val_loss: 4.6734\n",
      "Epoch 91/1000\n",
      "11/11 [==============================] - 2s 156ms/step - loss: 5.9925 - val_loss: 4.6399\n",
      "Epoch 92/1000\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 5.9385 - val_loss: 4.7023\n",
      "Epoch 93/1000\n",
      "11/11 [==============================] - 2s 193ms/step - loss: 5.9595 - val_loss: 4.6553\n",
      "Epoch 94/1000\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 5.9218 - val_loss: 4.6969\n",
      "Epoch 95/1000\n",
      "11/11 [==============================] - 2s 175ms/step - loss: 5.9105 - val_loss: 4.6998\n",
      "Epoch 96/1000\n",
      "11/11 [==============================] - 2s 193ms/step - loss: 5.8414 - val_loss: 4.6531\n",
      "Epoch 97/1000\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 5.8505 - val_loss: 4.6956\n",
      "Epoch 98/1000\n",
      "11/11 [==============================] - 2s 180ms/step - loss: 5.8111 - val_loss: 4.6661\n",
      "Epoch 99/1000\n",
      "11/11 [==============================] - 2s 184ms/step - loss: 5.8022 - val_loss: 4.6275\n",
      "Epoch 100/1000\n",
      "11/11 [==============================] - 2s 206ms/step - loss: 5.8019 - val_loss: 4.7128\n",
      "Epoch 101/1000\n",
      "11/11 [==============================] - 2s 188ms/step - loss: 5.7527 - val_loss: 4.8046\n",
      "Epoch 102/1000\n",
      "11/11 [==============================] - 2s 186ms/step - loss: 5.7403 - val_loss: 4.6111\n",
      "Epoch 103/1000\n",
      "11/11 [==============================] - 2s 184ms/step - loss: 5.7256 - val_loss: 4.6101\n",
      "Epoch 104/1000\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 5.6877 - val_loss: 4.6160\n",
      "Epoch 105/1000\n",
      "11/11 [==============================] - 2s 185ms/step - loss: 5.7113 - val_loss: 4.6019\n",
      "Epoch 106/1000\n",
      "11/11 [==============================] - 3s 233ms/step - loss: 5.6410 - val_loss: 4.5806\n",
      "Epoch 107/1000\n",
      "11/11 [==============================] - 3s 235ms/step - loss: 5.6386 - val_loss: 4.7231\n",
      "Epoch 108/1000\n",
      "11/11 [==============================] - 3s 295ms/step - loss: 5.6326 - val_loss: 4.6378\n",
      "Epoch 109/1000\n",
      "11/11 [==============================] - 2s 206ms/step - loss: 5.5755 - val_loss: 4.6473\n",
      "Epoch 110/1000\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 5.5974 - val_loss: 4.6539\n",
      "Epoch 111/1000\n",
      "11/11 [==============================] - 3s 252ms/step - loss: 5.4901 - val_loss: 4.7136\n",
      "Epoch 112/1000\n",
      "11/11 [==============================] - 3s 278ms/step - loss: 5.5752 - val_loss: 4.6566\n",
      "Epoch 113/1000\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 5.5157 - val_loss: 4.5503\n",
      "Epoch 114/1000\n",
      "11/11 [==============================] - 3s 241ms/step - loss: 5.5060 - val_loss: 4.5777\n",
      "Epoch 115/1000\n",
      "11/11 [==============================] - 2s 207ms/step - loss: 5.4210 - val_loss: 4.6345\n",
      "Epoch 116/1000\n",
      "11/11 [==============================] - 3s 231ms/step - loss: 5.4781 - val_loss: 4.5585\n",
      "Epoch 117/1000\n",
      "11/11 [==============================] - 3s 292ms/step - loss: 5.5133 - val_loss: 4.5485\n",
      "Epoch 118/1000\n",
      "11/11 [==============================] - 2s 224ms/step - loss: 5.4327 - val_loss: 4.5717\n",
      "Epoch 119/1000\n",
      "11/11 [==============================] - 3s 307ms/step - loss: 5.4062 - val_loss: 4.5583\n",
      "Epoch 120/1000\n",
      "11/11 [==============================] - 4s 323ms/step - loss: 5.3758 - val_loss: 4.6817\n",
      "Epoch 121/1000\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 5.3980 - val_loss: 4.6646\n",
      "Epoch 122/1000\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 5.4101 - val_loss: 4.6192\n",
      "Epoch 123/1000\n",
      "11/11 [==============================] - 3s 241ms/step - loss: 5.3798 - val_loss: 4.5235\n",
      "Epoch 124/1000\n",
      "11/11 [==============================] - 2s 201ms/step - loss: 5.3417 - val_loss: 4.5208\n",
      "Epoch 125/1000\n",
      "11/11 [==============================] - 2s 214ms/step - loss: 5.3378 - val_loss: 4.6427\n",
      "Epoch 126/1000\n",
      "11/11 [==============================] - 3s 242ms/step - loss: 5.3169 - val_loss: 4.6415\n",
      "Epoch 127/1000\n",
      "11/11 [==============================] - 3s 287ms/step - loss: 5.2797 - val_loss: 4.5499\n",
      "Epoch 128/1000\n",
      "11/11 [==============================] - 4s 322ms/step - loss: 5.2999 - val_loss: 4.5519\n",
      "Epoch 129/1000\n",
      "11/11 [==============================] - 3s 260ms/step - loss: 5.2786 - val_loss: 4.5161\n",
      "Epoch 130/1000\n",
      "11/11 [==============================] - 3s 239ms/step - loss: 5.2493 - val_loss: 4.4960\n",
      "Epoch 131/1000\n",
      "11/11 [==============================] - 2s 218ms/step - loss: 5.2170 - val_loss: 4.5458\n",
      "Epoch 132/1000\n",
      "11/11 [==============================] - 2s 224ms/step - loss: 5.2541 - val_loss: 4.4590\n",
      "Epoch 133/1000\n",
      "11/11 [==============================] - 3s 260ms/step - loss: 5.2169 - val_loss: 4.5213\n",
      "Epoch 134/1000\n",
      "11/11 [==============================] - 3s 265ms/step - loss: 5.1725 - val_loss: 4.4938\n",
      "Epoch 135/1000\n",
      "11/11 [==============================] - 3s 269ms/step - loss: 5.1349 - val_loss: 4.4797\n",
      "Epoch 136/1000\n",
      "11/11 [==============================] - 3s 293ms/step - loss: 5.1485 - val_loss: 4.5183\n",
      "Epoch 137/1000\n",
      "11/11 [==============================] - 3s 313ms/step - loss: 5.1147 - val_loss: 4.5527\n",
      "Epoch 138/1000\n",
      "11/11 [==============================] - 3s 274ms/step - loss: 5.1682 - val_loss: 4.5472\n",
      "Epoch 139/1000\n",
      "11/11 [==============================] - 3s 248ms/step - loss: 5.0684 - val_loss: 4.5099\n",
      "Epoch 140/1000\n",
      "11/11 [==============================] - 2s 219ms/step - loss: 5.0939 - val_loss: 4.5525\n",
      "Epoch 141/1000\n",
      "11/11 [==============================] - 2s 202ms/step - loss: 5.0803 - val_loss: 4.4954\n",
      "Epoch 142/1000\n",
      "11/11 [==============================] - 2s 202ms/step - loss: 5.0190 - val_loss: 4.4376\n",
      "Epoch 143/1000\n",
      "11/11 [==============================] - 2s 208ms/step - loss: 5.0586 - val_loss: 4.5082\n",
      "Epoch 144/1000\n",
      "11/11 [==============================] - 3s 286ms/step - loss: 5.0514 - val_loss: 4.5414\n",
      "Epoch 145/1000\n",
      "11/11 [==============================] - 2s 201ms/step - loss: 4.9968 - val_loss: 4.6535\n",
      "Epoch 146/1000\n",
      "11/11 [==============================] - 3s 278ms/step - loss: 5.0524 - val_loss: 4.5484\n",
      "Epoch 147/1000\n",
      "11/11 [==============================] - 3s 272ms/step - loss: 5.0447 - val_loss: 4.4791\n",
      "Epoch 148/1000\n",
      "11/11 [==============================] - 3s 239ms/step - loss: 4.9871 - val_loss: 4.4677\n",
      "Epoch 149/1000\n",
      "11/11 [==============================] - 2s 203ms/step - loss: 4.9644 - val_loss: 4.5459\n",
      "Epoch 150/1000\n",
      "11/11 [==============================] - 3s 260ms/step - loss: 4.9627 - val_loss: 4.4447\n",
      "Epoch 151/1000\n",
      "11/11 [==============================] - 2s 211ms/step - loss: 4.9966 - val_loss: 4.5015\n",
      "Epoch 152/1000\n",
      "11/11 [==============================] - 2s 200ms/step - loss: 4.9615 - val_loss: 4.4229\n",
      "Epoch 153/1000\n",
      "11/11 [==============================] - 2s 202ms/step - loss: 4.9636 - val_loss: 4.4303\n",
      "Epoch 154/1000\n",
      "11/11 [==============================] - 2s 205ms/step - loss: 4.9188 - val_loss: 4.4910\n",
      "Epoch 155/1000\n",
      "11/11 [==============================] - 2s 201ms/step - loss: 4.9104 - val_loss: 4.5543\n",
      "Epoch 156/1000\n",
      "11/11 [==============================] - 2s 204ms/step - loss: 4.8972 - val_loss: 4.5422\n",
      "Epoch 157/1000\n",
      "11/11 [==============================] - 2s 202ms/step - loss: 4.9531 - val_loss: 4.5165\n",
      "Epoch 158/1000\n",
      "11/11 [==============================] - 2s 195ms/step - loss: 4.8349 - val_loss: 4.4834\n",
      "Epoch 159/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 2s 182ms/step - loss: 4.8291 - val_loss: 4.4235\n",
      "Epoch 160/1000\n",
      "11/11 [==============================] - 2s 187ms/step - loss: 4.8519 - val_loss: 4.4412\n",
      "Epoch 161/1000\n",
      "11/11 [==============================] - 2s 189ms/step - loss: 4.7952 - val_loss: 4.4595\n",
      "Epoch 162/1000\n",
      "11/11 [==============================] - 2s 197ms/step - loss: 4.8204 - val_loss: 4.4200\n",
      "Epoch 163/1000\n",
      "11/11 [==============================] - 2s 204ms/step - loss: 4.7779 - val_loss: 4.4341\n",
      "Epoch 164/1000\n",
      "11/11 [==============================] - 3s 238ms/step - loss: 4.7720 - val_loss: 4.3858\n",
      "Epoch 165/1000\n",
      "11/11 [==============================] - 2s 216ms/step - loss: 4.8097 - val_loss: 4.4037\n",
      "Epoch 166/1000\n",
      "11/11 [==============================] - 2s 200ms/step - loss: 4.7455 - val_loss: 4.4044\n",
      "Epoch 167/1000\n",
      "11/11 [==============================] - 2s 198ms/step - loss: 4.7458 - val_loss: 4.5363\n",
      "Epoch 168/1000\n",
      "11/11 [==============================] - 2s 213ms/step - loss: 4.7322 - val_loss: 4.4605\n",
      "Epoch 169/1000\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 4.6974 - val_loss: 4.4405\n",
      "Epoch 170/1000\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 4.7177 - val_loss: 4.5475\n",
      "Epoch 171/1000\n",
      "11/11 [==============================] - 2s 211ms/step - loss: 4.7109 - val_loss: 4.5084\n",
      "Epoch 172/1000\n",
      "11/11 [==============================] - 3s 274ms/step - loss: 4.7337 - val_loss: 4.5774\n",
      "Epoch 173/1000\n",
      "11/11 [==============================] - 2s 189ms/step - loss: 4.6962 - val_loss: 4.4781\n",
      "Epoch 174/1000\n",
      "11/11 [==============================] - 2s 200ms/step - loss: 4.6964 - val_loss: 4.4822\n",
      "Epoch 175/1000\n",
      "11/11 [==============================] - 2s 201ms/step - loss: 4.6804 - val_loss: 4.4644\n",
      "Epoch 176/1000\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 4.6653 - val_loss: 4.4997\n",
      "Epoch 177/1000\n",
      "11/11 [==============================] - 2s 190ms/step - loss: 4.6284 - val_loss: 4.3744\n",
      "Epoch 178/1000\n",
      "11/11 [==============================] - 2s 176ms/step - loss: 4.6342 - val_loss: 4.5108\n",
      "Epoch 179/1000\n",
      "11/11 [==============================] - 2s 208ms/step - loss: 4.6492 - val_loss: 4.4461\n",
      "Epoch 180/1000\n",
      "11/11 [==============================] - 2s 198ms/step - loss: 4.6498 - val_loss: 4.3934\n",
      "Epoch 181/1000\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 4.6088 - val_loss: 4.4036\n",
      "Epoch 182/1000\n",
      "11/11 [==============================] - 2s 195ms/step - loss: 4.6088 - val_loss: 4.3971\n",
      "Epoch 183/1000\n",
      "11/11 [==============================] - 2s 214ms/step - loss: 4.6047 - val_loss: 4.4440\n",
      "Epoch 184/1000\n",
      "11/11 [==============================] - 2s 207ms/step - loss: 4.5663 - val_loss: 4.4535\n",
      "Epoch 185/1000\n",
      "11/11 [==============================] - 2s 181ms/step - loss: 4.5589 - val_loss: 4.3548\n",
      "Epoch 186/1000\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 4.5783 - val_loss: 4.5011\n",
      "Epoch 187/1000\n",
      "11/11 [==============================] - 4s 332ms/step - loss: 4.5414 - val_loss: 4.5639\n",
      "Epoch 188/1000\n",
      "11/11 [==============================] - 4s 364ms/step - loss: 4.5854 - val_loss: 4.4797\n",
      "Epoch 189/1000\n",
      "11/11 [==============================] - 3s 286ms/step - loss: 4.5578 - val_loss: 4.3899\n",
      "Epoch 190/1000\n",
      "11/11 [==============================] - 2s 215ms/step - loss: 4.5389 - val_loss: 4.3685\n",
      "Epoch 191/1000\n",
      "11/11 [==============================] - 2s 193ms/step - loss: 4.5635 - val_loss: 4.5295\n",
      "Epoch 192/1000\n",
      "11/11 [==============================] - 2s 190ms/step - loss: 4.5446 - val_loss: 4.4841\n",
      "Epoch 193/1000\n",
      "11/11 [==============================] - 2s 188ms/step - loss: 4.5448 - val_loss: 4.3620\n",
      "Epoch 194/1000\n",
      "11/11 [==============================] - 2s 195ms/step - loss: 4.5323 - val_loss: 4.3751\n",
      "Epoch 195/1000\n",
      "11/11 [==============================] - 2s 207ms/step - loss: 4.5335 - val_loss: 4.4635\n",
      "Epoch 196/1000\n",
      "11/11 [==============================] - 3s 251ms/step - loss: 4.4801 - val_loss: 4.4469\n",
      "Epoch 197/1000\n",
      "11/11 [==============================] - 2s 218ms/step - loss: 4.4867 - val_loss: 4.3883\n",
      "Epoch 198/1000\n",
      "11/11 [==============================] - 2s 200ms/step - loss: 4.5075 - val_loss: 4.3611\n",
      "Epoch 199/1000\n",
      "11/11 [==============================] - 3s 227ms/step - loss: 4.4982 - val_loss: 4.4702\n",
      "Epoch 200/1000\n",
      "11/11 [==============================] - 2s 223ms/step - loss: 4.4578 - val_loss: 4.4518\n",
      "Epoch 201/1000\n",
      "11/11 [==============================] - 3s 240ms/step - loss: 4.4594 - val_loss: 4.4381\n",
      "Epoch 202/1000\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 4.4683 - val_loss: 4.3822\n",
      "Epoch 203/1000\n",
      "11/11 [==============================] - 3s 245ms/step - loss: 4.4600 - val_loss: 4.4473\n",
      "Epoch 204/1000\n",
      "11/11 [==============================] - 3s 250ms/step - loss: 4.4464 - val_loss: 4.5775\n",
      "Epoch 205/1000\n",
      "11/11 [==============================] - 3s 251ms/step - loss: 4.4736 - val_loss: 4.4238\n",
      "Epoch 206/1000\n",
      "11/11 [==============================] - 3s 236ms/step - loss: 4.4371 - val_loss: 4.3786\n",
      "Epoch 207/1000\n",
      "11/11 [==============================] - 2s 219ms/step - loss: 4.4462 - val_loss: 4.4895\n",
      "Epoch 208/1000\n",
      "11/11 [==============================] - 3s 261ms/step - loss: 4.4120 - val_loss: 4.4405\n",
      "Epoch 209/1000\n",
      "11/11 [==============================] - 3s 260ms/step - loss: 4.4264 - val_loss: 4.3517\n",
      "Epoch 210/1000\n",
      "11/11 [==============================] - 2s 210ms/step - loss: 4.4121 - val_loss: 4.5169\n",
      "Epoch 211/1000\n",
      "11/11 [==============================] - 2s 208ms/step - loss: 4.4479 - val_loss: 4.4174\n",
      "Epoch 212/1000\n",
      "11/11 [==============================] - 3s 236ms/step - loss: 4.4072 - val_loss: 4.3367\n",
      "Epoch 213/1000\n",
      "11/11 [==============================] - 2s 227ms/step - loss: 4.3988 - val_loss: 4.3549\n",
      "Epoch 214/1000\n",
      "11/11 [==============================] - 2s 207ms/step - loss: 4.4070 - val_loss: 4.4520\n",
      "Epoch 215/1000\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 4.4065 - val_loss: 4.4789\n",
      "Epoch 216/1000\n",
      "11/11 [==============================] - 2s 206ms/step - loss: 4.4070 - val_loss: 4.4226\n",
      "Epoch 217/1000\n",
      "11/11 [==============================] - 2s 200ms/step - loss: 4.3889 - val_loss: 4.3421\n",
      "Epoch 218/1000\n",
      "11/11 [==============================] - 2s 200ms/step - loss: 4.3502 - val_loss: 4.4849\n",
      "Epoch 219/1000\n",
      "11/11 [==============================] - 2s 205ms/step - loss: 4.3674 - val_loss: 4.3556\n",
      "Epoch 220/1000\n",
      "11/11 [==============================] - 2s 203ms/step - loss: 4.3940 - val_loss: 4.3603\n",
      "Epoch 221/1000\n",
      "11/11 [==============================] - 2s 198ms/step - loss: 4.3714 - val_loss: 4.4568\n",
      "Epoch 222/1000\n",
      "11/11 [==============================] - 2s 201ms/step - loss: 4.3581 - val_loss: 4.3363\n",
      "Epoch 223/1000\n",
      "11/11 [==============================] - 3s 228ms/step - loss: 4.3555 - val_loss: 4.4188\n",
      "Epoch 224/1000\n",
      "11/11 [==============================] - 2s 193ms/step - loss: 4.3841 - val_loss: 4.3805\n",
      "Epoch 225/1000\n",
      "11/11 [==============================] - 2s 200ms/step - loss: 4.3550 - val_loss: 4.4145\n",
      "Epoch 226/1000\n",
      "11/11 [==============================] - 2s 202ms/step - loss: 4.3517 - val_loss: 4.3877\n",
      "Epoch 227/1000\n",
      "11/11 [==============================] - 2s 211ms/step - loss: 4.3663 - val_loss: 4.5114\n",
      "Epoch 228/1000\n",
      "11/11 [==============================] - 3s 240ms/step - loss: 4.3505 - val_loss: 4.4395\n",
      "Epoch 229/1000\n",
      "11/11 [==============================] - 2s 221ms/step - loss: 4.3511 - val_loss: 4.4475\n",
      "Epoch 230/1000\n",
      "11/11 [==============================] - 3s 246ms/step - loss: 4.3881 - val_loss: 4.4191\n",
      "Epoch 231/1000\n",
      "11/11 [==============================] - 3s 315ms/step - loss: 4.3550 - val_loss: 4.5135\n",
      "Epoch 232/1000\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 4.3604 - val_loss: 4.4540\n",
      "Epoch 233/1000\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 4.3257 - val_loss: 4.3464\n",
      "Epoch 234/1000\n",
      "11/11 [==============================] - 4s 390ms/step - loss: 4.3530 - val_loss: 4.3911\n",
      "Epoch 235/1000\n",
      "11/11 [==============================] - 3s 280ms/step - loss: 4.3617 - val_loss: 4.4093\n",
      "Epoch 236/1000\n",
      "11/11 [==============================] - 3s 260ms/step - loss: 4.3084 - val_loss: 4.4550\n",
      "Epoch 237/1000\n",
      "11/11 [==============================] - 3s 268ms/step - loss: 4.3486 - val_loss: 4.4554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 238/1000\n",
      "11/11 [==============================] - 2s 220ms/step - loss: 4.3085 - val_loss: 4.4511\n",
      "Epoch 239/1000\n",
      "11/11 [==============================] - 3s 261ms/step - loss: 4.3204 - val_loss: 4.2761\n",
      "Epoch 240/1000\n",
      "11/11 [==============================] - 3s 268ms/step - loss: 4.3485 - val_loss: 4.5589\n",
      "Epoch 241/1000\n",
      "11/11 [==============================] - 2s 214ms/step - loss: 4.3217 - val_loss: 4.3979\n",
      "Epoch 242/1000\n",
      "11/11 [==============================] - 2s 197ms/step - loss: 4.3242 - val_loss: 4.4369\n",
      "Epoch 243/1000\n",
      "11/11 [==============================] - 2s 203ms/step - loss: 4.3116 - val_loss: 4.3624\n",
      "Epoch 244/1000\n",
      "11/11 [==============================] - 2s 203ms/step - loss: 4.3096 - val_loss: 4.3403\n",
      "Epoch 245/1000\n",
      "11/11 [==============================] - 2s 209ms/step - loss: 4.3072 - val_loss: 4.3595\n",
      "Epoch 246/1000\n",
      "11/11 [==============================] - 2s 195ms/step - loss: 4.3073 - val_loss: 4.3712\n",
      "Epoch 247/1000\n",
      "11/11 [==============================] - 2s 206ms/step - loss: 4.3002 - val_loss: 4.4065\n",
      "Epoch 248/1000\n",
      "11/11 [==============================] - 2s 201ms/step - loss: 4.2989 - val_loss: 4.3465\n",
      "Epoch 249/1000\n",
      "11/11 [==============================] - 2s 202ms/step - loss: 4.2729 - val_loss: 4.4716\n",
      "Epoch 250/1000\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 4.3157 - val_loss: 4.3908\n",
      "Epoch 251/1000\n",
      "11/11 [==============================] - 2s 201ms/step - loss: 4.2913 - val_loss: 4.2983\n",
      "Epoch 252/1000\n",
      "11/11 [==============================] - 2s 195ms/step - loss: 4.3121 - val_loss: 4.3787\n",
      "Epoch 253/1000\n",
      "11/11 [==============================] - 2s 200ms/step - loss: 4.2702 - val_loss: 4.4020\n",
      "Epoch 254/1000\n",
      "11/11 [==============================] - 2s 218ms/step - loss: 4.2772 - val_loss: 4.4919\n",
      "Epoch 255/1000\n",
      "11/11 [==============================] - 3s 283ms/step - loss: 4.2877 - val_loss: 4.4502\n",
      "Epoch 256/1000\n",
      "11/11 [==============================] - 3s 299ms/step - loss: 4.2904 - val_loss: 4.3798\n",
      "Epoch 257/1000\n",
      "11/11 [==============================] - 3s 281ms/step - loss: 4.2659 - val_loss: 4.3961\n",
      "Epoch 258/1000\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 4.2628 - val_loss: 4.4850\n",
      "Epoch 259/1000\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 4.2790 - val_loss: 4.5529\n",
      "Epoch 260/1000\n",
      "11/11 [==============================] - 3s 228ms/step - loss: 4.3067 - val_loss: 4.3149\n",
      "Epoch 261/1000\n",
      "11/11 [==============================] - 3s 234ms/step - loss: 4.2946 - val_loss: 4.3435\n",
      "Epoch 262/1000\n",
      "11/11 [==============================] - 3s 245ms/step - loss: 4.2623 - val_loss: 4.4614\n",
      "Epoch 263/1000\n",
      "11/11 [==============================] - 3s 280ms/step - loss: 4.2539 - val_loss: 4.4213\n",
      "Epoch 264/1000\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 4.2593 - val_loss: 4.2901\n",
      "Epoch 265/1000\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 4.2728 - val_loss: 4.3818\n",
      "Epoch 266/1000\n",
      "11/11 [==============================] - 2s 199ms/step - loss: 4.2824 - val_loss: 4.4467\n",
      "Epoch 267/1000\n",
      "11/11 [==============================] - 2s 223ms/step - loss: 4.2464 - val_loss: 4.3774\n",
      "Epoch 268/1000\n",
      "11/11 [==============================] - 2s 200ms/step - loss: 4.2562 - val_loss: 4.3674\n",
      "Epoch 269/1000\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 4.2463 - val_loss: 4.5506\n",
      "Epoch 270/1000\n",
      "11/11 [==============================] - 2s 188ms/step - loss: 4.2669 - val_loss: 4.3910\n",
      "Epoch 271/1000\n",
      "11/11 [==============================] - 2s 211ms/step - loss: 4.2422 - val_loss: 4.4133\n",
      "Epoch 272/1000\n",
      "11/11 [==============================] - 2s 202ms/step - loss: 4.2556 - val_loss: 4.3709\n",
      "Epoch 273/1000\n",
      "11/11 [==============================] - 2s 200ms/step - loss: 4.2649 - val_loss: 4.3803\n",
      "Epoch 274/1000\n",
      "11/11 [==============================] - 2s 200ms/step - loss: 4.2472 - val_loss: 4.3218\n",
      "Epoch 275/1000\n",
      "11/11 [==============================] - 3s 238ms/step - loss: 4.2490 - val_loss: 4.4739\n",
      "Epoch 276/1000\n",
      "11/11 [==============================] - 3s 265ms/step - loss: 4.2723 - val_loss: 4.4988\n",
      "Epoch 277/1000\n",
      "11/11 [==============================] - 2s 210ms/step - loss: 4.2847 - val_loss: 4.4049\n",
      "Epoch 278/1000\n",
      "11/11 [==============================] - 2s 197ms/step - loss: 4.2313 - val_loss: 4.4612\n",
      "Epoch 279/1000\n",
      "11/11 [==============================] - 2s 205ms/step - loss: 4.2404 - val_loss: 4.3315\n",
      "Epoch 280/1000\n",
      "11/11 [==============================] - 2s 184ms/step - loss: 4.2217 - val_loss: 4.3175\n",
      "Epoch 281/1000\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 4.2650 - val_loss: 4.3408\n",
      "Epoch 282/1000\n",
      "11/11 [==============================] - 2s 206ms/step - loss: 4.2617 - val_loss: 4.3055\n",
      "Epoch 283/1000\n",
      "11/11 [==============================] - 3s 280ms/step - loss: 4.2415 - val_loss: 4.4324\n",
      "Epoch 284/1000\n",
      "11/11 [==============================] - 4s 395ms/step - loss: 4.2426 - val_loss: 4.3815\n",
      "Epoch 285/1000\n",
      "11/11 [==============================] - 3s 297ms/step - loss: 4.2146 - val_loss: 4.3400\n",
      "Epoch 286/1000\n",
      "11/11 [==============================] - 2s 218ms/step - loss: 4.2222 - val_loss: 4.3508\n",
      "Epoch 287/1000\n",
      "11/11 [==============================] - 2s 189ms/step - loss: 4.2270 - val_loss: 4.3548\n",
      "Epoch 288/1000\n",
      "11/11 [==============================] - 2s 187ms/step - loss: 4.2430 - val_loss: 4.2919\n",
      "Epoch 289/1000\n",
      "11/11 [==============================] - 2s 206ms/step - loss: 4.2444 - val_loss: 4.3821\n",
      "Epoch 290/1000\n",
      "11/11 [==============================] - 3s 273ms/step - loss: 4.2354 - val_loss: 4.5064\n",
      "Epoch 291/1000\n",
      "11/11 [==============================] - 3s 233ms/step - loss: 4.2275 - val_loss: 4.3776\n",
      "Epoch 292/1000\n",
      "11/11 [==============================] - 2s 223ms/step - loss: 4.2267 - val_loss: 4.3071\n",
      "Epoch 293/1000\n",
      "11/11 [==============================] - 4s 332ms/step - loss: 4.2103 - val_loss: 4.4256\n",
      "Epoch 294/1000\n",
      "11/11 [==============================] - 3s 244ms/step - loss: 4.2340 - val_loss: 4.3530\n",
      "Epoch 295/1000\n",
      "11/11 [==============================] - 2s 223ms/step - loss: 4.1904 - val_loss: 4.2362\n",
      "Epoch 296/1000\n",
      "11/11 [==============================] - 2s 213ms/step - loss: 4.2768 - val_loss: 4.6017\n",
      "Epoch 297/1000\n",
      "11/11 [==============================] - 2s 219ms/step - loss: 4.2293 - val_loss: 4.3756\n",
      "Epoch 298/1000\n",
      "11/11 [==============================] - 2s 220ms/step - loss: 4.2050 - val_loss: 4.3405\n",
      "Epoch 299/1000\n",
      "11/11 [==============================] - 3s 244ms/step - loss: 4.2101 - val_loss: 4.4075\n",
      "Epoch 300/1000\n",
      "11/11 [==============================] - 3s 236ms/step - loss: 4.2186 - val_loss: 4.3887\n",
      "Epoch 301/1000\n",
      "11/11 [==============================] - 2s 220ms/step - loss: 4.2120 - val_loss: 4.4755\n",
      "Epoch 302/1000\n",
      "11/11 [==============================] - 2s 218ms/step - loss: 4.2291 - val_loss: 4.3186\n",
      "Epoch 303/1000\n",
      "11/11 [==============================] - 2s 220ms/step - loss: 4.2032 - val_loss: 4.3410\n",
      "Epoch 304/1000\n",
      "11/11 [==============================] - 3s 248ms/step - loss: 4.2173 - val_loss: 4.3917\n",
      "Epoch 305/1000\n",
      "11/11 [==============================] - 3s 275ms/step - loss: 4.1902 - val_loss: 4.3483\n",
      "Epoch 306/1000\n",
      "11/11 [==============================] - 2s 223ms/step - loss: 4.1839 - val_loss: 4.3502\n",
      "Epoch 307/1000\n",
      "11/11 [==============================] - 3s 280ms/step - loss: 4.1899 - val_loss: 4.4437\n",
      "Epoch 308/1000\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 4.1904 - val_loss: 4.3672\n",
      "Epoch 309/1000\n",
      "11/11 [==============================] - 3s 260ms/step - loss: 4.2040 - val_loss: 4.3676\n",
      "Epoch 310/1000\n",
      "11/11 [==============================] - 3s 277ms/step - loss: 4.1960 - val_loss: 4.3200\n",
      "Epoch 311/1000\n",
      "11/11 [==============================] - 3s 296ms/step - loss: 4.2064 - val_loss: 4.3919\n",
      "Epoch 312/1000\n",
      "11/11 [==============================] - 3s 286ms/step - loss: 4.1920 - val_loss: 4.4308\n",
      "Epoch 313/1000\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 4.1971 - val_loss: 4.3211\n",
      "Epoch 314/1000\n",
      "11/11 [==============================] - 3s 269ms/step - loss: 4.1876 - val_loss: 4.2718\n",
      "Epoch 315/1000\n",
      "11/11 [==============================] - 3s 237ms/step - loss: 4.2040 - val_loss: 4.3726\n",
      "Epoch 316/1000\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 4.1986 - val_loss: 4.4206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 317/1000\n",
      "11/11 [==============================] - 3s 275ms/step - loss: 4.1830 - val_loss: 4.4869\n",
      "Epoch 318/1000\n",
      "11/11 [==============================] - 3s 262ms/step - loss: 4.1912 - val_loss: 4.2574\n",
      "Epoch 319/1000\n",
      "11/11 [==============================] - 3s 273ms/step - loss: 4.2004 - val_loss: 4.3193\n",
      "Epoch 320/1000\n",
      "11/11 [==============================] - 3s 255ms/step - loss: 4.1790 - val_loss: 4.4234\n",
      "Epoch 321/1000\n",
      "11/11 [==============================] - 3s 261ms/step - loss: 4.1783 - val_loss: 4.3366\n",
      "Epoch 322/1000\n",
      "11/11 [==============================] - 3s 294ms/step - loss: 4.1851 - val_loss: 4.3464\n",
      "Epoch 323/1000\n",
      "11/11 [==============================] - 3s 291ms/step - loss: 4.1807 - val_loss: 4.3088\n",
      "Epoch 324/1000\n",
      "11/11 [==============================] - 2s 218ms/step - loss: 4.1772 - val_loss: 4.3394\n",
      "Epoch 325/1000\n",
      "11/11 [==============================] - 3s 228ms/step - loss: 4.1526 - val_loss: 4.3979\n",
      "Epoch 326/1000\n",
      "11/11 [==============================] - 3s 249ms/step - loss: 4.1745 - val_loss: 4.2816\n",
      "Epoch 327/1000\n",
      "11/11 [==============================] - 3s 242ms/step - loss: 4.1609 - val_loss: 4.3146\n",
      "Epoch 328/1000\n",
      "11/11 [==============================] - 3s 239ms/step - loss: 4.1451 - val_loss: 4.3258\n",
      "Epoch 329/1000\n",
      "11/11 [==============================] - 2s 220ms/step - loss: 4.1688 - val_loss: 4.3636\n",
      "Epoch 330/1000\n",
      "11/11 [==============================] - 2s 225ms/step - loss: 4.1547 - val_loss: 4.4239\n",
      "Epoch 331/1000\n",
      "11/11 [==============================] - 3s 244ms/step - loss: 4.1999 - val_loss: 4.4774\n",
      "Epoch 332/1000\n",
      "11/11 [==============================] - 2s 223ms/step - loss: 4.1918 - val_loss: 4.2805\n",
      "Epoch 333/1000\n",
      "11/11 [==============================] - 2s 225ms/step - loss: 4.1513 - val_loss: 4.3631\n",
      "Epoch 334/1000\n",
      "11/11 [==============================] - 2s 201ms/step - loss: 4.1658 - val_loss: 4.4622\n",
      "Epoch 335/1000\n",
      "11/11 [==============================] - 2s 226ms/step - loss: 4.1932 - val_loss: 4.2891\n",
      "Epoch 336/1000\n",
      "11/11 [==============================] - 3s 232ms/step - loss: 4.1586 - val_loss: 4.3241\n",
      "Epoch 337/1000\n",
      "11/11 [==============================] - 2s 203ms/step - loss: 4.1481 - val_loss: 4.3516\n",
      "Epoch 338/1000\n",
      "11/11 [==============================] - 2s 208ms/step - loss: 4.1589 - val_loss: 4.2776\n",
      "Epoch 339/1000\n",
      "11/11 [==============================] - 3s 283ms/step - loss: 4.1634 - val_loss: 4.3346\n",
      "Epoch 340/1000\n",
      "11/11 [==============================] - 3s 267ms/step - loss: 4.1670 - val_loss: 4.4141\n",
      "Epoch 341/1000\n",
      "11/11 [==============================] - 3s 298ms/step - loss: 4.1652 - val_loss: 4.3767\n",
      "Epoch 342/1000\n",
      "11/11 [==============================] - 4s 330ms/step - loss: 4.1428 - val_loss: 4.3832\n",
      "Epoch 343/1000\n",
      "11/11 [==============================] - 3s 262ms/step - loss: 4.1521 - val_loss: 4.3159\n",
      "Epoch 344/1000\n",
      "11/11 [==============================] - 2s 216ms/step - loss: 4.1271 - val_loss: 4.3694\n",
      "Epoch 345/1000\n",
      "11/11 [==============================] - 3s 232ms/step - loss: 4.1704 - val_loss: 4.3429\n",
      "Epoch 346/1000\n",
      "11/11 [==============================] - 3s 253ms/step - loss: 4.1222 - val_loss: 4.3903\n",
      "Epoch 347/1000\n",
      "11/11 [==============================] - 3s 265ms/step - loss: 4.1641 - val_loss: 4.4837\n",
      "Epoch 348/1000\n",
      "11/11 [==============================] - 2s 213ms/step - loss: 4.1414 - val_loss: 4.2882\n",
      "Epoch 349/1000\n",
      "11/11 [==============================] - 3s 252ms/step - loss: 4.1327 - val_loss: 4.4019\n",
      "Epoch 350/1000\n",
      "11/11 [==============================] - 4s 385ms/step - loss: 4.1513 - val_loss: 4.3792\n",
      "Epoch 351/1000\n",
      "11/11 [==============================] - 4s 350ms/step - loss: 4.1303 - val_loss: 4.3541\n",
      "Epoch 352/1000\n",
      "11/11 [==============================] - 3s 274ms/step - loss: 4.1499 - val_loss: 4.3267\n",
      "Epoch 353/1000\n",
      "11/11 [==============================] - 3s 272ms/step - loss: 4.1399 - val_loss: 4.2907\n",
      "Epoch 354/1000\n",
      "11/11 [==============================] - 3s 261ms/step - loss: 4.1222 - val_loss: 4.3536\n",
      "Epoch 355/1000\n",
      "11/11 [==============================] - 2s 211ms/step - loss: 4.1407 - val_loss: 4.4103\n",
      "Epoch 356/1000\n",
      "11/11 [==============================] - 3s 230ms/step - loss: 4.1446 - val_loss: 4.3046\n",
      "Epoch 357/1000\n",
      "11/11 [==============================] - 2s 208ms/step - loss: 4.1543 - val_loss: 4.2619\n",
      "Epoch 358/1000\n",
      "11/11 [==============================] - 2s 209ms/step - loss: 4.1572 - val_loss: 4.4161\n",
      "Epoch 359/1000\n",
      "11/11 [==============================] - 3s 238ms/step - loss: 4.1636 - val_loss: 4.3789\n",
      "Epoch 360/1000\n",
      "11/11 [==============================] - 2s 208ms/step - loss: 4.1452 - val_loss: 4.3417\n",
      "Epoch 361/1000\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 4.1498 - val_loss: 4.2608\n",
      "Epoch 362/1000\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 4.1310 - val_loss: 4.3305\n",
      "Epoch 363/1000\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 4.1476 - val_loss: 4.3552\n",
      "Epoch 364/1000\n",
      "11/11 [==============================] - 3s 240ms/step - loss: 4.1282 - val_loss: 4.3651\n",
      "Epoch 365/1000\n",
      "11/11 [==============================] - 2s 200ms/step - loss: 4.1295 - val_loss: 4.2877\n",
      "Epoch 366/1000\n",
      "11/11 [==============================] - 2s 193ms/step - loss: 4.1311 - val_loss: 4.2724\n",
      "Epoch 367/1000\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 4.1162 - val_loss: 4.3983\n",
      "Epoch 368/1000\n",
      "11/11 [==============================] - 2s 197ms/step - loss: 4.1342 - val_loss: 4.3374\n",
      "Epoch 369/1000\n",
      "11/11 [==============================] - 3s 239ms/step - loss: 4.1270 - val_loss: 4.2821\n",
      "Epoch 370/1000\n",
      "11/11 [==============================] - 2s 202ms/step - loss: 4.1277 - val_loss: 4.4019\n",
      "Epoch 371/1000\n",
      "11/11 [==============================] - 2s 198ms/step - loss: 4.1238 - val_loss: 4.2989\n",
      "Epoch 372/1000\n",
      "11/11 [==============================] - 2s 198ms/step - loss: 4.1251 - val_loss: 4.3023\n",
      "Epoch 373/1000\n",
      "11/11 [==============================] - 3s 298ms/step - loss: 4.1334 - val_loss: 4.4102\n",
      "Epoch 374/1000\n",
      "11/11 [==============================] - 3s 250ms/step - loss: 4.1190 - val_loss: 4.4542\n",
      "Epoch 375/1000\n",
      "11/11 [==============================] - 3s 243ms/step - loss: 4.1267 - val_loss: 4.3201\n",
      "Epoch 376/1000\n",
      "11/11 [==============================] - 3s 239ms/step - loss: 4.1297 - val_loss: 4.3382\n",
      "Epoch 377/1000\n",
      "11/11 [==============================] - 2s 224ms/step - loss: 4.1251 - val_loss: 4.3617\n",
      "Epoch 378/1000\n",
      "11/11 [==============================] - 3s 276ms/step - loss: 4.1283 - val_loss: 4.2353\n",
      "Epoch 379/1000\n",
      "11/11 [==============================] - 2s 218ms/step - loss: 4.1579 - val_loss: 4.2924\n",
      "Epoch 380/1000\n",
      "11/11 [==============================] - 2s 207ms/step - loss: 4.0951 - val_loss: 4.3589\n",
      "Epoch 381/1000\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 4.1242 - val_loss: 4.3106\n",
      "Epoch 382/1000\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 4.1180 - val_loss: 4.3710\n",
      "Epoch 383/1000\n",
      "11/11 [==============================] - 2s 205ms/step - loss: 4.1385 - val_loss: 4.2843\n",
      "Epoch 384/1000\n",
      "11/11 [==============================] - 3s 273ms/step - loss: 4.1124 - val_loss: 4.3583\n",
      "Epoch 385/1000\n",
      "11/11 [==============================] - 2s 224ms/step - loss: 4.1048 - val_loss: 4.3544\n",
      "Epoch 386/1000\n",
      "11/11 [==============================] - 3s 235ms/step - loss: 4.1311 - val_loss: 4.3508\n",
      "Epoch 387/1000\n",
      "11/11 [==============================] - 2s 218ms/step - loss: 4.0871 - val_loss: 4.2980\n",
      "Epoch 388/1000\n",
      "11/11 [==============================] - 2s 223ms/step - loss: 4.0964 - val_loss: 4.2330\n",
      "Epoch 389/1000\n",
      "11/11 [==============================] - 2s 226ms/step - loss: 4.1191 - val_loss: 4.4225\n",
      "Epoch 390/1000\n",
      "11/11 [==============================] - 3s 241ms/step - loss: 4.1127 - val_loss: 4.2983\n",
      "Epoch 391/1000\n",
      "11/11 [==============================] - 2s 222ms/step - loss: 4.1087 - val_loss: 4.3432\n",
      "Epoch 392/1000\n",
      "11/11 [==============================] - 3s 236ms/step - loss: 4.1109 - val_loss: 4.2221\n",
      "Epoch 393/1000\n",
      "11/11 [==============================] - 2s 209ms/step - loss: 4.1232 - val_loss: 4.4665\n",
      "Epoch 394/1000\n",
      "11/11 [==============================] - 3s 313ms/step - loss: 4.1172 - val_loss: 4.2860\n",
      "Epoch 395/1000\n",
      "11/11 [==============================] - 3s 289ms/step - loss: 4.1275 - val_loss: 4.2143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 396/1000\n",
      "11/11 [==============================] - 3s 229ms/step - loss: 4.1008 - val_loss: 4.3535\n",
      "Epoch 397/1000\n",
      "11/11 [==============================] - 3s 250ms/step - loss: 4.1099 - val_loss: 4.3750\n",
      "Epoch 398/1000\n",
      "11/11 [==============================] - 3s 299ms/step - loss: 4.1302 - val_loss: 4.3464\n",
      "Epoch 399/1000\n",
      "11/11 [==============================] - 3s 284ms/step - loss: 4.1424 - val_loss: 4.2999\n",
      "Epoch 400/1000\n",
      "11/11 [==============================] - 3s 267ms/step - loss: 4.1388 - val_loss: 4.2884\n",
      "Epoch 401/1000\n",
      "11/11 [==============================] - 3s 258ms/step - loss: 4.1355 - val_loss: 4.3759\n",
      "Epoch 402/1000\n",
      "11/11 [==============================] - 3s 262ms/step - loss: 4.1084 - val_loss: 4.3361\n",
      "Epoch 403/1000\n",
      "11/11 [==============================] - 2s 209ms/step - loss: 4.1016 - val_loss: 4.3360\n",
      "Epoch 404/1000\n",
      "11/11 [==============================] - 2s 200ms/step - loss: 4.1167 - val_loss: 4.2800\n",
      "Epoch 405/1000\n",
      "11/11 [==============================] - 2s 205ms/step - loss: 4.1059 - val_loss: 4.3803\n",
      "Epoch 406/1000\n",
      "11/11 [==============================] - 2s 226ms/step - loss: 4.0828 - val_loss: 4.2908\n",
      "Epoch 407/1000\n",
      "11/11 [==============================] - 3s 282ms/step - loss: 4.1044 - val_loss: 4.2534\n",
      "Epoch 408/1000\n",
      "11/11 [==============================] - 3s 235ms/step - loss: 4.1036 - val_loss: 4.2736\n",
      "Epoch 409/1000\n",
      "11/11 [==============================] - 2s 198ms/step - loss: 4.1035 - val_loss: 4.3196\n",
      "Epoch 410/1000\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 4.1102 - val_loss: 4.3403\n",
      "Epoch 411/1000\n",
      "11/11 [==============================] - 2s 190ms/step - loss: 4.0943 - val_loss: 4.3496\n",
      "Epoch 412/1000\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 4.1078 - val_loss: 4.2810\n",
      "Epoch 413/1000\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 4.1089 - val_loss: 4.3500\n",
      "Epoch 414/1000\n",
      "11/11 [==============================] - 2s 176ms/step - loss: 4.0971 - val_loss: 4.3890\n",
      "Epoch 415/1000\n",
      "11/11 [==============================] - 2s 193ms/step - loss: 4.0779 - val_loss: 4.2674\n",
      "Epoch 416/1000\n",
      "11/11 [==============================] - 2s 197ms/step - loss: 4.0910 - val_loss: 4.2734\n",
      "Epoch 417/1000\n",
      "11/11 [==============================] - 2s 189ms/step - loss: 4.0812 - val_loss: 4.4247\n",
      "Epoch 418/1000\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 4.1016 - val_loss: 4.2691\n",
      "Epoch 419/1000\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 4.0970 - val_loss: 4.2806\n",
      "Epoch 420/1000\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 4.1130 - val_loss: 4.4822\n",
      "Epoch 421/1000\n",
      "11/11 [==============================] - 2s 176ms/step - loss: 4.1075 - val_loss: 4.3097\n",
      "Epoch 422/1000\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 4.1035 - val_loss: 4.2496\n",
      "Epoch 423/1000\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 4.1379 - val_loss: 4.4356\n",
      "Epoch 424/1000\n",
      "11/11 [==============================] - 2s 197ms/step - loss: 4.0862 - val_loss: 4.3902\n",
      "Epoch 425/1000\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 4.0988 - val_loss: 4.2804\n",
      "Epoch 426/1000\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 4.0658 - val_loss: 4.2353\n",
      "Epoch 427/1000\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 4.0784 - val_loss: 4.3189\n",
      "Epoch 428/1000\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 4.0986 - val_loss: 4.3863\n",
      "Epoch 429/1000\n",
      "11/11 [==============================] - 2s 178ms/step - loss: 4.0773 - val_loss: 4.2548\n",
      "Epoch 430/1000\n",
      "11/11 [==============================] - 2s 165ms/step - loss: 4.0768 - val_loss: 4.3567\n",
      "Epoch 431/1000\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 4.0968 - val_loss: 4.2700\n",
      "Epoch 432/1000\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 4.0897 - val_loss: 4.3940\n",
      "Epoch 433/1000\n",
      "11/11 [==============================] - 2s 183ms/step - loss: 4.1038 - val_loss: 4.3773\n",
      "Epoch 434/1000\n",
      "11/11 [==============================] - 2s 193ms/step - loss: 4.0782 - val_loss: 4.2556\n",
      "Epoch 435/1000\n",
      "11/11 [==============================] - 2s 190ms/step - loss: 4.0995 - val_loss: 4.3040\n",
      "Epoch 436/1000\n",
      "11/11 [==============================] - 2s 189ms/step - loss: 4.0639 - val_loss: 4.5343\n",
      "Epoch 437/1000\n",
      "11/11 [==============================] - 2s 195ms/step - loss: 4.1203 - val_loss: 4.3290\n",
      "Epoch 438/1000\n",
      "11/11 [==============================] - 2s 195ms/step - loss: 4.0826 - val_loss: 4.2227\n",
      "Epoch 439/1000\n",
      "11/11 [==============================] - 2s 195ms/step - loss: 4.0953 - val_loss: 4.3454\n",
      "Epoch 440/1000\n",
      "11/11 [==============================] - 2s 197ms/step - loss: 4.0921 - val_loss: 4.2977\n",
      "Epoch 441/1000\n",
      "11/11 [==============================] - 2s 186ms/step - loss: 4.0783 - val_loss: 4.2569\n",
      "Epoch 442/1000\n",
      "11/11 [==============================] - 2s 163ms/step - loss: 4.0938 - val_loss: 4.3621\n",
      "Epoch 443/1000\n",
      "11/11 [==============================] - 2s 179ms/step - loss: 4.0857 - val_loss: 4.3135\n",
      "Epoch 444/1000\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 4.0836 - val_loss: 4.2433\n",
      "Epoch 445/1000\n",
      "11/11 [==============================] - 2s 184ms/step - loss: 4.1266 - val_loss: 4.2930\n",
      "Epoch 446/1000\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 4.0975 - val_loss: 4.3137\n",
      "Epoch 447/1000\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 4.0616 - val_loss: 4.3007\n",
      "Epoch 448/1000\n",
      "11/11 [==============================] - 2s 195ms/step - loss: 4.1053 - val_loss: 4.2869\n",
      "Epoch 449/1000\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 4.0720 - val_loss: 4.3289\n",
      "Epoch 450/1000\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 4.0887 - val_loss: 4.3462\n",
      "Epoch 451/1000\n",
      "11/11 [==============================] - 2s 162ms/step - loss: 4.0664 - val_loss: 4.3187\n",
      "Epoch 452/1000\n",
      "11/11 [==============================] - 2s 167ms/step - loss: 4.0819 - val_loss: 4.3377\n",
      "Epoch 453/1000\n",
      "11/11 [==============================] - 2s 195ms/step - loss: 4.0734 - val_loss: 4.3078\n",
      "Epoch 454/1000\n",
      "11/11 [==============================] - 2s 189ms/step - loss: 4.0539 - val_loss: 4.3473\n",
      "Epoch 455/1000\n",
      "11/11 [==============================] - 2s 192ms/step - loss: 4.0668 - val_loss: 4.3948\n",
      "Epoch 456/1000\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 4.0612 - val_loss: 4.3246\n",
      "Epoch 457/1000\n",
      "11/11 [==============================] - 2s 197ms/step - loss: 4.0630 - val_loss: 4.3740\n",
      "Epoch 458/1000\n",
      "11/11 [==============================] - 2s 181ms/step - loss: 4.0772 - val_loss: 4.4824\n",
      "Epoch 459/1000\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 4.0784 - val_loss: 4.2980\n",
      "Epoch 460/1000\n",
      "11/11 [==============================] - 2s 195ms/step - loss: 4.0684 - val_loss: 4.3559\n",
      "Epoch 461/1000\n",
      "11/11 [==============================] - 2s 197ms/step - loss: 4.0530 - val_loss: 4.4417\n",
      "Epoch 462/1000\n",
      "11/11 [==============================] - 2s 185ms/step - loss: 4.0629 - val_loss: 4.2542\n",
      "Epoch 463/1000\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 4.0593 - val_loss: 4.3522\n",
      "Epoch 464/1000\n",
      "11/11 [==============================] - 3s 237ms/step - loss: 4.0583 - val_loss: 4.3045\n",
      "Epoch 465/1000\n",
      "11/11 [==============================] - 3s 279ms/step - loss: 4.0818 - val_loss: 4.2283\n",
      "Epoch 466/1000\n",
      "11/11 [==============================] - 3s 285ms/step - loss: 4.0635 - val_loss: 4.3157\n",
      "Epoch 467/1000\n",
      "11/11 [==============================] - 4s 330ms/step - loss: 4.0347 - val_loss: 4.3405\n",
      "Epoch 468/1000\n",
      "11/11 [==============================] - 3s 254ms/step - loss: 4.0667 - val_loss: 4.4195\n",
      "Epoch 469/1000\n",
      "11/11 [==============================] - 2s 227ms/step - loss: 4.0966 - val_loss: 4.4075\n",
      "Epoch 470/1000\n",
      "11/11 [==============================] - 2s 207ms/step - loss: 4.0628 - val_loss: 4.3428\n",
      "Epoch 471/1000\n",
      "11/11 [==============================] - 3s 241ms/step - loss: 4.0724 - val_loss: 4.2828\n",
      "Epoch 472/1000\n",
      "11/11 [==============================] - 2s 217ms/step - loss: 4.0587 - val_loss: 4.2357\n",
      "Epoch 473/1000\n",
      "11/11 [==============================] - 2s 184ms/step - loss: 4.0700 - val_loss: 4.3552\n",
      "Epoch 474/1000\n",
      "11/11 [==============================] - 2s 179ms/step - loss: 4.0624 - val_loss: 4.3606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 475/1000\n",
      "11/11 [==============================] - 3s 304ms/step - loss: 4.0548 - val_loss: 4.2661\n",
      "Epoch 476/1000\n",
      "11/11 [==============================] - 3s 297ms/step - loss: 4.0676 - val_loss: 4.2508\n",
      "Epoch 477/1000\n",
      "11/11 [==============================] - 3s 261ms/step - loss: 4.0573 - val_loss: 4.3424\n",
      "Epoch 478/1000\n",
      "11/11 [==============================] - 3s 234ms/step - loss: 4.0891 - val_loss: 4.2813\n",
      "Epoch 479/1000\n",
      "11/11 [==============================] - 3s 232ms/step - loss: 4.0695 - val_loss: 4.2682\n",
      "Epoch 480/1000\n",
      "11/11 [==============================] - 2s 191ms/step - loss: 4.0580 - val_loss: 4.2916\n",
      "Epoch 481/1000\n",
      "11/11 [==============================] - 2s 214ms/step - loss: 4.0486 - val_loss: 4.3003\n",
      "Epoch 482/1000\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 4.0470 - val_loss: 4.3028\n",
      "Epoch 483/1000\n",
      "11/11 [==============================] - 3s 233ms/step - loss: 4.0648 - val_loss: 4.3617\n",
      "Epoch 484/1000\n",
      "11/11 [==============================] - 2s 198ms/step - loss: 4.0718 - val_loss: 4.2495\n",
      "Epoch 485/1000\n",
      "11/11 [==============================] - 2s 196ms/step - loss: 4.0682 - val_loss: 4.2589\n",
      "Epoch 486/1000\n",
      "11/11 [==============================] - 2s 186ms/step - loss: 4.0638 - val_loss: 4.3774\n",
      "Epoch 487/1000\n",
      "11/11 [==============================] - 2s 194ms/step - loss: 4.0606 - val_loss: 4.3760\n",
      "Epoch 488/1000\n",
      "11/11 [==============================] - 3s 251ms/step - loss: 4.0612 - val_loss: 4.2410\n",
      "Epoch 489/1000\n",
      "11/11 [==============================] - 3s 249ms/step - loss: 4.0729 - val_loss: 4.3083\n",
      "Epoch 490/1000\n",
      "11/11 [==============================] - 3s 268ms/step - loss: 4.0612 - val_loss: 4.4107\n",
      "Epoch 491/1000\n",
      "11/11 [==============================] - 3s 256ms/step - loss: 4.0688 - val_loss: 4.2509\n",
      "Epoch 492/1000\n",
      "11/11 [==============================] - 2s 209ms/step - loss: 4.0594 - val_loss: 4.2951\n",
      "Epoch 493/1000\n",
      "11/11 [==============================] - 2s 206ms/step - loss: 4.0456 - val_loss: 4.3816\n",
      "Epoch 494/1000\n",
      "11/11 [==============================] - 2s 217ms/step - loss: 4.0358 - val_loss: 4.3821\n",
      "Epoch 495/1000\n",
      "11/11 [==============================] - 2s 214ms/step - loss: 4.0817 - val_loss: 4.2463\n",
      "Epoch 496/1000\n",
      "11/11 [==============================] - 3s 240ms/step - loss: 4.0514 - val_loss: 4.2541\n",
      "Epoch 497/1000\n",
      "11/11 [==============================] - 3s 268ms/step - loss: 4.0439 - val_loss: 4.3015\n",
      "Epoch 498/1000\n",
      " 9/11 [=======================>......] - ETA: 0s - loss: 4.0227"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9e02c6489526>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mMAX_EPOCHS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m history = model.fit(inputs,outputs,\n\u001b[0m\u001b[1;32m      3\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutputs_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \"\"\"\n\u001b[0;32m-> 1843\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MAX_EPOCHS=1000\n",
    "history = model.fit(inputs,outputs,\n",
    "                    batch_size=300, \n",
    "                    epochs=MAX_EPOCHS, \n",
    "                    validation_data=(inputs_val,outputs_val), \n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "json.dump(history_dict, open(\"../data/training/history_model.json\", 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"../data/sn_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = np.load(\"../data/padded_x_val.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = data_test[:,:-out_steps,:2], data_test[:,-out_steps:, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "maes = []\n",
    "for i in range(out_steps):\n",
    "    maes.append(mean_absolute_error(y_test[:,i,:], y_hat[:,i,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'MAE on Test')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(maes)\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.xlabel(\"$t+h$\")\n",
    "plt.title(\"MAE on Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_train = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "maes_train = []\n",
    "for i in range(out_steps):\n",
    "    maes_train.append(mean_absolute_error(y_train[:,i,:], y_hat_train[:,i,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'MAE on Train')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(maes_train)\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.xlabel(\"$t+h$\")\n",
    "plt.title(\"MAE on Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
